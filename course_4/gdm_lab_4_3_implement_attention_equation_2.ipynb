{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT2aH0n7nOe8"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb58ds8ynT3V"
      },
      "source": [
        "![](https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C4-white-bg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y9VEMjundtt"
      },
      "source": [
        "# Lab: Implement Masked Multi-Head Attention\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_4/gdm_lab_4_3_implement_attention_equation_2.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Compute the attention weight vector for multiple heads and apply masking to prevent the model from \"looking into the future.\"\n",
        "\n",
        "25 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7fZa-AxFuRl"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this lab, you will implement **masked multi-head attention**. As in the previous lab, you will use the learned projection matrices $W_Q$, $W_K$, $W_V$ from the Gemma-1B model and use these three matrices to compute the attention weights and the output of the attention mechanism.\n",
        "\n",
        "To simplify things, the previous two labs only considered the first attention head of the Gemma-1B model. In this lab, you can also visualize the other attention heads. In the case of Gemma-1B, there are four attention heads per layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKi7nGKIrC9M"
      },
      "source": [
        "### What you will learn\n",
        "\n",
        "By the end of this lab, you will:\n",
        "\n",
        "* Understand how you can compute the attention weights for multiple heads in parallel.\n",
        "* Recognize the role of masking attention weights in producing the final attention output.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "In this lab, you will:\n",
        "\n",
        "* Implement a function to create an attention mask.\n",
        "* Implement the masked multi-head attention mechanism.\n",
        "* Visualize and compare your attention weights with the attention weights of the reference implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qZfb7Qgzygc"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7z0omIO0Cek"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over the cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c_4t3O5z6Ba"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-qh65sxz7Gp"
      },
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz1Jd4yNoH92"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will mainly work with the JAX package to modify vectors and matrices. You will also use functions from the custom `ai_foundations` package to load Gemma, extract its parameters and to generate visualizations.\n",
        "\n",
        "Run the following cell to import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcRWNBIzOJ-i"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install orbax-checkpoint==0.11.21\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import jax # For working with vectors and matrices.\n",
        "import jax.numpy as jnp # For working with vectors and matrices.\n",
        "from ai_foundations import generation # For prompting the Gemma model.\n",
        "from ai_foundations import visualizations # For visualizing attention weights.\n",
        "from ai_foundations import attention # For working with Q,K,V matrices.\n",
        "# For providing feedback on your implementation.\n",
        "from ai_foundations.feedback.course_4 import attention as attention_feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8mSub9MoLb9"
      },
      "source": [
        "## Load the model and visualize attention weights\n",
        "\n",
        "You will use the parameters from the Gemma-1B model to compute the attention weights.\n",
        "\n",
        "Run the following cell to load the Gemma-1B parameters and initialize the caches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oafcqwAVTH2D"
      },
      "outputs": [],
      "source": [
        "# Initialize caches for prompts.\n",
        "previous_prompt = None\n",
        "previous_prompt2 = None\n",
        "\n",
        "# Load special version of the Gemma-1B model that provides access to attention\n",
        "# weights and QKV matrices.\n",
        "print(\"Loading Gemma-1B...\")\n",
        "model = generation.load_gemma(\"Gemma-1B-AttentionWeight\")\n",
        "print(\"Loaded Gemma-1B.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdB7VKCChgjQ"
      },
      "source": [
        "As in the previous labs, the following cell allows you to visualize the Gemma attention weights. Now you can also set which head should be considered. You can again use this cell as a reference and compare your own implementation to the output of the reference implementation. This will ensure that you have implemented everything accurately.\n",
        "\n",
        "Make sure to run this cell for at least one prompt as it also extracts the query, key, and value matrices that you will use in other cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2mJNICDcxgWH"
      },
      "outputs": [],
      "source": [
        "# @title Visualize attention weights (reference implementation)\n",
        "from IPython.display import clear_output\n",
        "\n",
        "layer = 19  # @param {type:\"slider\", min: 0, max: 25}\n",
        "\n",
        "head = 3  # @param {type:\"slider\", min: 0, max: 3}\n",
        "\n",
        "prompt = \"Jide was hungry so she went looking for\"  # @param {type: \"string\"}\n",
        "# @markdown Check the following box to display the attention weights for all tokens, not just for the generated one:\n",
        "show_all_weights = True  # @param {type:\"boolean\"}\n",
        "\n",
        "if prompt != previous_prompt:\n",
        "    (\n",
        "        output_text,\n",
        "        next_token_logits,\n",
        "        tokenizer,\n",
        "        attention_weights,\n",
        "        _,\n",
        "        qkv_dict,\n",
        "    ) = generation.prompt_attention_transformer_model(\n",
        "        prompt, model, sampling_mode=\"greedy\"\n",
        "    )\n",
        "    tokens = [tokenizer.tokens[t] for t in tokenizer.encode(output_text)]\n",
        "    previous_prompt = prompt\n",
        "\n",
        "print(f\"Generated text: {output_text}\")\n",
        "\n",
        "visualizations.visualize_attention(\n",
        "    tokens,\n",
        "    attention_weights[f\"layer_{layer}\"],\n",
        "    layer,\n",
        "    head=head,\n",
        "    min_line_thickness=0,\n",
        "    max_line_thickness=5,\n",
        "    show_all_weights=show_all_weights,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYbz6BWKl_ph"
      },
      "source": [
        "## Coding Activity 1: Attention mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqmFF9IY-lgk"
      },
      "source": [
        "------\n",
        ">**ðŸ’» Your task:**\n",
        ">\n",
        ">Complete the implementation of `compute_attention_mask` for computing the attention mask. The attention mask is a square triangular matrix where every value on or below the diagonal is 1 and every value above the diagonal is 0. Consider using [`jnp.tri`](https://docs.jax.dev/en/latest/_autosummary/jax.numpy.tri.html) for generating this matrix.\n",
        ">\n",
        ">Once you are done, run the following two cells to test your implementation. It should print the attention masks for inputs of lengths 5 and 6.\n",
        ">\n",
        "------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgNJxOXrTJjY"
      },
      "outputs": [],
      "source": [
        "def compute_attention_mask(num_tokens: int) -> jax.Array:\n",
        "    \"\"\"Computes the attention mask for an input of length `num_tokens`.\n",
        "\n",
        "    Args:\n",
        "      num_tokens: Number of tokens in the input.\n",
        "\n",
        "    Returns:\n",
        "      attention_mask: A diagonal matrix indicating which attention logit should\n",
        "        be masked. Shape: (num_tokens, num_tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    attention_mask = ...  # Compute the attention mask here.\n",
        "\n",
        "    return attention_mask\n",
        "\n",
        "print(f\"Attention mask for input length 5:\\n {compute_attention_mask(5)}\")\n",
        "print(f\"\\nAttention mask for input length 6:\\n {compute_attention_mask(6)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WrhGvet3jb4p"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your implementation of `compute_attention_mask`.\n",
        "attention_feedback.test_compute_attention_mask(compute_attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehey0n4gmR8K"
      },
      "source": [
        "## Implement multi-head attention\n",
        "\n",
        "During the next five activities, you will now implement the multi-head attention mechanism step-by-step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJhaRbhAO6bt"
      },
      "source": [
        "### Coding Activity 2: Stack matrices\n",
        "\n",
        "------\n",
        ">**ðŸ’» Your task:**\n",
        ">\n",
        "> As a first step, you will prepare the query, key, and value projections such that you can easily compute the attention weights for all heads in parallel. To do this, complete the `stack_matrices` function in the following cell.\n",
        ">\n",
        "> The query, key, and value projections are stored in a list where each list entry is one projection matrix for one head (e.g., the first list entry in all three lists corresponds to the $Q$, $K$, and $V$ matrices for the first attention head). `stack_matrices` should stack these matrices together such that you obtain a 3-dimensional tensor (i.e., a matrix but with 3 dimensions) for $Q$, $K$, and $V$. The first dimension should correspond to the head, the second dimension to the token index, and the third dimension should be individual components of the vectors. Consider using the [`jnp.stack`](https://docs.jax.dev/en/latest/_autosummary/jax.numpy.stack.html) function for stacking matrices.\n",
        ">\n",
        "> Complete the implementation in the next cell and test your implementation by running the cell after that.\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KITab93wL4kA"
      },
      "outputs": [],
      "source": [
        "def stack_matrices(\n",
        "    qkv_dict: dict[str, dict[str, jax.Array]], layer: int\n",
        ") -> tuple[jax.Array, jax.Array, jax.Array]:\n",
        "    \"\"\"Retrieves the list of query, key, and value matrices for each head and\n",
        "        stacks them into a multi-dimensional tensor.\n",
        "\n",
        "    Args:\n",
        "      qkv_dict: A dictionary containing the raw query, key, and value\n",
        "        projections for all layers. The keys are strings identifying the layers\n",
        "        and matrix types, and values are the corresponding JAX arrays.\n",
        "      layer: The specific layer for which to compute the attention weights.\n",
        "\n",
        "    Returns:\n",
        "      query_proj: Query projection tensor.\n",
        "        Shape: (num_heads, num_tokens, dim_k).\n",
        "      key_proj: Key projection tensor. Shape: (num_heads, num_tokens, dim_k).\n",
        "      value_proj: Value projection tensor.\n",
        "        Shape: (num_heads, num_tokens, dim_v).\n",
        "    \"\"\"\n",
        "\n",
        "    # Passing `head=None` results in `get_qkv_matrices` returning lists of\n",
        "    # `num_heads` projection matrices rather than just an individual projection\n",
        "    # matrix.\n",
        "    (\n",
        "        query_proj_list,\n",
        "        key_proj_list,\n",
        "        value_proj_list\n",
        "    ) = attention.get_qkv_matrices(\n",
        "        qkv_dict, layer, head=None\n",
        "    )\n",
        "\n",
        "    # Stack the query, key, and value projection matrices here:\n",
        "    query_proj = ...\n",
        "    key_proj = ...\n",
        "    value_proj = ...\n",
        "\n",
        "    return query_proj, key_proj, value_proj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hNOSkGjbra_X"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your implementation of `stack_matrices`.\n",
        "attention_feedback.test_stack_matrices(stack_matrices, qkv_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efS3yS9OmVHu"
      },
      "source": [
        "### Coding Activity 3: Compute raw logits\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> As the next step, complete the function `compute_raw_logits` to compute the logits using the query and key matrix.\n",
        ">\n",
        "> This function needs to implement three operations:\n",
        "> 1. Compute $d_k$, the dimension of the key vectors which you need to compute the normalization factor. Remember that you can use [`.shape`](https://docs.jax.dev/en/latest/_autosummary/jax.Array.shape.html) for this.\n",
        "> 2. Compute the transposition of the key projection matrix, $K^T$. This is already implemented.\n",
        "> 3. Compute the logits. You get these from performing a matrix multiplication between the query projections and the key projections and then normalizing them by the square root of $d_k$:\n",
        ">\n",
        ">$$\\mbox{logits} = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
        ">\n",
        "> Complete the implementation in the next cell and test your implementation by running the cell after that.\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4S09fo97_tX"
      },
      "outputs": [],
      "source": [
        "def compute_raw_logits(query_proj: jax.Array, key_proj: jax.Array) -> jax.Array:\n",
        "    \"\"\"Computes the raw logits from the query and key projection tensors, before\n",
        "    applying the attention mask.\n",
        "\n",
        "    Args:\n",
        "      query_proj: The query projection tensor.\n",
        "        Shape: (num_heads, num_tokens, dim_k).\n",
        "      key_proj: The key projection tensor.\n",
        "        Shape: (num_heads, num_tokens, dim_k).\n",
        "\n",
        "    Returns:\n",
        "      logits: The raw logits. Shape: (num_heads, num_tokens, num_tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    # Transpose the last two dimensions of the key matrix for matrix\n",
        "    # multiplication. The shape changes from (num_heads, num_tokens, dim_k) to\n",
        "    # (num_heads, dim_k, num_tokens).\n",
        "    key_transposed = jnp.transpose(key_proj, (0, 2, 1))\n",
        "\n",
        "    logits = ... # Compute the raw logits here.\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MRXJxglxCsQs"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your implementation of `compute_raw_logits`.\n",
        "attention_feedback.test_compute_raw_logits(compute_raw_logits, qkv_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R38dkCQjmZHf"
      },
      "source": [
        "### Coding Activity 4: Apply attention mask\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Complete the function `apply_attention_mask` to compute the attention mask and apply it to the raw logits from the previous step.\n",
        ">\n",
        "> This function needs to implement two operations:\n",
        "> 1. Compute the attention mask using your `compute_attention_mask` function.\n",
        "> 2. Compute the masked logits:\n",
        ">   $$\\mbox{logits_masked}_{i,j} = \\begin{cases}\\mbox{logits}_{i,j} & \\mbox{if mask}_{i,j} = 1 \\\\ K & \\mbox{otherwise}\\end{cases}$$\n",
        ">   To compute a function that depends on a condition like this, you can use the [`jnp.where`](https://docs.jax.dev/en/latest/_autosummary/jax.numpy.where.html) function. $K$ is defined in the next cell as `K_MASK`.\n",
        ">\n",
        ">\n",
        "> Complete the implementation in the next cell and test your implementation by running the cell after that.\n",
        ">\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEi1gHf57_b4"
      },
      "outputs": [],
      "source": [
        "K_MASK = -2.3819763e+38\n",
        "\n",
        "def apply_attention_mask(logits_raw: jax.Array) -> jax.Array:\n",
        "    \"\"\"Computes the attention mask (using `compute_attention_mask`) and\n",
        "    applies it to the logits.\n",
        "\n",
        "    Args:\n",
        "      logits_raw: Raw logits. Shape: (num_heads, num_tokens, num_tokens).\n",
        "\n",
        "    Returns:\n",
        "      Masked logits. Shape: (num_heads, num_tokens, num_tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    _, num_tokens, _ = logits_raw.shape\n",
        "\n",
        "    attention_mask = ... # Compute the attention mask here.\n",
        "    logits_masked = ... # Compute the masked logits here.\n",
        "\n",
        "    return logits_masked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CvO63FGkFLtj"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your implementation of `apply_attention_mask`.\n",
        "attention_feedback.test_apply_attention_mask(apply_attention_mask, qkv_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOL5_C5_mfhH"
      },
      "source": [
        "### Coding Activity 5: Compute attention weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReQVtJbbxYrO"
      },
      "source": [
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Complete the function `compute_attention_weights` to compute the attention weights `alpha`.\n",
        ">\n",
        "> `alpha` is computed by applying the SoftMax function to the logits. You can either implement your own SoftMax function or use [`jax.nn.softmax`](https://docs.jax.dev/en/latest/_autosummary/jax.nn.softmax.html):\n",
        "$$\\alpha = \\mbox{SoftMax}(\\mbox{logits})$$\n",
        ">\n",
        "> Note that by default, `jax.nn.softmax` normalizes values in a multi-dimensional matrix or tensor such that all values across the last dimension sum up to 1. This is the correct behavior here but in other cases, you may have to specify a different dimension using the `axis` argument.\n",
        ">\n",
        "> Complete the implementation in the next cell and test your implementation by running the cell after that.\n",
        ">\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y08a3ywR7_Sw"
      },
      "outputs": [],
      "source": [
        "def compute_attention_weights(logits_masked: jax.Array) -> jax.Array:\n",
        "    \"\"\"Computes the attention weights `alpha` from the masked logits by applying\n",
        "    the SoftMax function.\n",
        "\n",
        "    Args:\n",
        "      logits_masked: Masked logits computed in previous steps.\n",
        "        Shape: (num_heads, num_tokens, num_tokens).\n",
        "\n",
        "    Returns:\n",
        "      alpha: The attention weights. Shape: (num_heads, num_tokens, num_tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    alpha = ... # Apply the SoftMax here.\n",
        "\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "w30t2HcXF_fB"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your implementation of `compute_attention_weights`.\n",
        "attention_feedback.test_compute_attention_weights(compute_attention_weights, qkv_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPJaByHpmi5n"
      },
      "source": [
        "### Coding Activity 6: Compute attention output\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Complete the function `compute_attention_output` to compute the output of the attention mechanism.\n",
        ">\n",
        "> Compute the output by multiplying the attention weights $\\alpha$ with the value projections:\n",
        "$$Y = \\alpha V$$\n",
        ">\n",
        "> Complete the implementation in the next cell and test your implementation by running the cell after that.\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_huZgB77_GC"
      },
      "outputs": [],
      "source": [
        "def compute_attention_output(\n",
        "    alpha: jax.Array, value_proj: jax.Array\n",
        ") -> jax.Array:\n",
        "    \"\"\"Computes the output of the attention mechanism by computing the weighted\n",
        "    sum of the embeddings in `value_proj`, using the attention weights `alpha`.\n",
        "\n",
        "    Args:\n",
        "      alpha: Attention weights. Shape: (num_heads, num_tokens, num_tokens).\n",
        "      value_proj: Tensor with value projections.\n",
        "        Shape: (num_heads, num_tokens, dim_v).\n",
        "\n",
        "    Returns:\n",
        "      Y: output of the attention mechanism.\n",
        "        Shape: (num_heads, num_tokens, dim_v).\n",
        "    \"\"\"\n",
        "\n",
        "    Y = ... # Compute the output of the attention mechanism.\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c5VlvSXjGM9U"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your implementation of `compute_attention_output`.\n",
        "attention_feedback.test_compute_attention_output(compute_attention_output, qkv_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGAl3tLjmsG2"
      },
      "source": [
        "### Putting it all together\n",
        "\n",
        "The following cell implements the `compute_attention` function that calls all the functions you have already implemented in order to perform the steps required to compute the output of the masked multi-head attention mechanism.\n",
        "\n",
        "Note that in this activity, you are skipping the projection step in which the output of each attention head is combined to the final output of the multi-head attention mechanism. Instead, `compute_attention` returns the output of each individual attention head.\n",
        "\n",
        "Run the following cell to define the function. It has already been fully implemented, so you do not need to add anything to this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELT78rSW7-9q"
      },
      "outputs": [],
      "source": [
        "def compute_attention(\n",
        "    qkv_dict: dict[str, dict[str, jax.Array]], layer: int\n",
        ") -> tuple[jax.Array, jax.Array]:\n",
        "    \"\"\"Implements the multi-head attention mechanism using the above-defined\n",
        "    functions to:\n",
        "      - stack the projection matrices\n",
        "      - compute the raw and masked logits\n",
        "      - compute the attention weights alpha\n",
        "      - compute the attention output Y.\n",
        "\n",
        "    Args:\n",
        "      qkv_dict: A dictionary containing the raw query, key, and value\n",
        "        projections for all layers. The keys are strings identifying the layers\n",
        "        and matrix types, and values are the corresponding JAX arrays.\n",
        "      layer: The specific layer for which to compute the attention weights.\n",
        "\n",
        "    Returns:\n",
        "      Y: Output of multi-head attention mechanism.\n",
        "        Shape: (num_tokens, num_heads, dim_v).\n",
        "      alpha: Attention weights for each head.\n",
        "        Shape: (num_tokens, num_heads, num_tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    # Stack the matrices.\n",
        "    # Shapes:\n",
        "    #   query_proj: (num_heads, num_tokens, dim_k).\n",
        "    #   key_proj: (num_heads, num_tokens, dim_k).\n",
        "    #   value_proj: (num_heads, num_tokens, dim_v).\n",
        "    query_proj, key_proj, value_proj = stack_matrices(qkv_dict, layer)\n",
        "\n",
        "    # Compute the raw logits. Shape: (num_heads, num_tokens, num_tokens).\n",
        "    logits_raw = compute_raw_logits(query_proj, key_proj)\n",
        "\n",
        "    # Compute the masked logits. Shape: (num_heads, num_tokens, num_tokens).\n",
        "    logits_masked = apply_attention_mask(logits_raw)\n",
        "\n",
        "    # Compute the attention weights. Shape: (num_heads, num_tokens, num_tokens).\n",
        "    alpha = compute_attention_weights(logits_masked)\n",
        "\n",
        "    # Compute the output of the attention mechanism.\n",
        "    # Shape: (num_heads, num_tokens, dim_v).\n",
        "    Y = compute_attention_output(alpha, value_proj)\n",
        "\n",
        "    # Transposes the matrix such that the first dimension is the token index and\n",
        "    # the second dimension is the head. This is to make the function compatible\n",
        "    # with how Gemma represents attention weight matrices.\n",
        "    # Shape: (num_tokens, num_heads, num_tokens).\n",
        "    alpha = jnp.transpose(alpha, (1, 0, 2))\n",
        "    Y = jnp.transpose(Y, (1, 0, 2)) # Shape: (num_tokens, num_heads, dim_v).\n",
        "\n",
        "    return Y, alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVb9XIaTm1PB"
      },
      "source": [
        "### Visualize your attention weights\n",
        "\n",
        "The following cell uses your implementation of the attention weights and visualizes the output of the `compute_attention` function above. Enter the same prompt as you entered above and compare the visualization of your attention weights to the original ones that were extracted from Gemma above.\n",
        "\n",
        "For the same layer, the same head, and the same prompt, both visualizations should be identical. If they are not, make sure to check your implementation above and try again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-Ol7vD-hMoqs"
      },
      "outputs": [],
      "source": [
        "# @title Visualize attention weights (your implementation)\n",
        "layer = 19  # @param {type:\"slider\", min: 0, max: 25}\n",
        "head = 3  # @param {type:\"slider\", min: 0, max: 3}\n",
        "\n",
        "prompt = \"Jide was hungry so she went looking for\"  # @param {type: 'string'}\n",
        "# @markdown Check the following box to display the attention weights for all tokens, not just for the generated one:\n",
        "show_all_weights = True  # @param {type:\"boolean\", label: \"sfdsf\"}\n",
        "\n",
        "model_name = \"Gemma-1B\"\n",
        "\n",
        "if prompt != previous_prompt2:\n",
        "    (\n",
        "        output_text2,\n",
        "        next_token_logits2,\n",
        "        tokenizer,\n",
        "        attention_weights2,\n",
        "        attention_mask2,\n",
        "        qkv_dict2,\n",
        "    ) = generation.prompt_attention_transformer_model(\n",
        "        prompt, model, sampling_mode=\"greedy\"\n",
        "    )\n",
        "    tokens2 = [tokenizer.tokens[t] for t in tokenizer.encode(output_text2)]\n",
        "    previous_prompt2 = prompt\n",
        "\n",
        "print(f\"Generated text: {output_text2}\")\n",
        "\n",
        "attn_out, attn_weights = compute_attention(qkv_dict2, layer)\n",
        "\n",
        "visualizations.visualize_attention(\n",
        "    tokens2,\n",
        "    attn_weights,\n",
        "    layer,\n",
        "    head=head,\n",
        "    min_line_thickness=0,\n",
        "    max_line_thickness=5,\n",
        "    show_all_weights=show_all_weights,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHSLaHiv673h"
      },
      "source": [
        "### Efficiently computing the attention weights\n",
        "\n",
        "The two visualizations in this lab show the attention weights for all tokens. As you have seen in this lab, the $Q$, $K$, and $V$ projection matrices contain the projections for all tokens and for all heads. Therefore, your computations automatically compute the attention weights and outputs for all tokens and heads. This allows for a high level of parallelization and makes transformer models very well-suited for efficiently training them on large quantities of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AjRCBdrRMDy"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you implemented the **masked multi-head attention mechanism**, which is the core mechanism of any transformer model. You used the `jnp.tri` method for constructing the attention mask and then combined the query, key, and value matrices for all heads to 3-dimensional tensors. Finally, you computed the attention weights for all heads and used them to compute the output of the attention mechanism. This provides you with contextual embeddings from which you can predict the next token for a prompt.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6ZHKCCKQ95P"
      },
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8juDjypQ_HF"
      },
      "source": [
        "### Coding Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbOBTFfYnIsT"
      },
      "outputs": [],
      "source": [
        "def compute_attention_mask(num_tokens: int) -> jax.Array:\n",
        "    \"\"\"Computes the attention mask for an input of length `num_tokens`.\n",
        "\n",
        "    Args:\n",
        "      num_tokens: Number of tokens in the input.\n",
        "\n",
        "    Returns:\n",
        "      attention_mask: A diagonal matrix indicating which attention logit should\n",
        "        be masked. Shape: (num_tokens, num_tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    attention_mask = jnp.tri(num_tokens) # Shape: (num_tokens, num_tokens).\n",
        "\n",
        "    return attention_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InwPXSMynMY2"
      },
      "source": [
        "### Coding Activity 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFc3nrsonJWf"
      },
      "outputs": [],
      "source": [
        "def stack_matrices(\n",
        "    qkv_dict: dict[str, dict[str, jax.Array]], layer: int\n",
        ") -> tuple[jax.Array, jax.Array, jax.Array]:\n",
        "    \"\"\"Retrieves the list of query, key, and value matrices for each head and\n",
        "        stacks them into a multi-dimensional tensor.\n",
        "\n",
        "    Args:\n",
        "      qkv_dict: A dictionary containing the raw query, key, and value\n",
        "        projections for all layers. The keys are strings identifying the layers\n",
        "        and matrix types, and values are the corresponding JAX arrays.\n",
        "      layer: The specific layer for which to compute the attention weights.\n",
        "\n",
        "    Returns:\n",
        "      query_proj: Query projection tensor.\n",
        "        Shape: (num_heads, num_tokens, dim_k).\n",
        "      key_proj: Key projection tensor. Shape: (num_heads, num_tokens, dim_k).\n",
        "      value_proj: Value projection tensor.\n",
        "        Shape: (num_heads, num_tokens, dim_v).\n",
        "    \"\"\"\n",
        "\n",
        "    # Passing `head=None` results in `get_qkv_matrices` returning lists of\n",
        "    # `num_heads` projection matrices rather than just an individual projection\n",
        "    # matrices.\n",
        "    (\n",
        "        query_proj_list,\n",
        "        key_proj_list,\n",
        "        value_proj_list\n",
        "    ) = attention.get_qkv_matrices(\n",
        "        qkv_dict, layer, head=None\n",
        "    )\n",
        "\n",
        "    # Stack the query, key, and value projection matrices here:\n",
        "    # Shape: (num_heads, num_tokens, dim_k).\n",
        "    query_proj = jnp.stack(query_proj_list, axis=0)\n",
        "    # Shape: (num_heads, num_tokens, dim_k).\n",
        "    key_proj = jnp.stack(key_proj_list, axis=0)\n",
        "    # Shape: (num_heads, num_tokens, dim_v).\n",
        "    value_proj = jnp.stack(value_proj_list, axis=0)\n",
        "\n",
        "    return query_proj, key_proj, value_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E0z2sR4nODQ"
      },
      "source": [
        "### Coding Activity 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL34Pw3RnODR"
      },
      "outputs": [],
      "source": [
        "def compute_raw_logits(query_proj: jax.Array, key_proj: jax.Array) -> jax.Array:\n",
        "    \"\"\"Computes the raw logits from the query and key projection tensors, before\n",
        "    applying the attention mask.\n",
        "\n",
        "    Args:\n",
        "      query_proj: The query projection tensor.\n",
        "        Shape: (num_heads, num_tokens, dim_k).\n",
        "      key_proj: The key projection tensor.\n",
        "        Shape: (num_heads, num_tokens, dim_k).\n",
        "\n",
        "    Returns:\n",
        "      logits: The raw logits. Shape: (num_heads, num_tokens, num_tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    # Transpose the last two dimensions of the key matrix for matrix\n",
        "    # multiplication. The shape changes from (heads, num_tokens, dim_k) to\n",
        "    # (num_heads, dim_k, num_tokens).\n",
        "    key_transposed = jnp.transpose(key_proj, (0, 2, 1))\n",
        "\n",
        "    _, _, dim_k = key_proj.shape\n",
        "    # Shape: (num_heads, num_tokens, num_tokens).\n",
        "    logits = query_proj @ key_transposed / jnp.sqrt(dim_k)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-ov4hLqnOYX"
      },
      "source": [
        "### Coding Activity 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGWwi3ihnOYY"
      },
      "outputs": [],
      "source": [
        "def apply_attention_mask(logits_raw: jax.Array) -> jax.Array:\n",
        "    \"\"\"Computes the attention mask (using `compute_attention_mask`) and\n",
        "    applies it to the logits.\n",
        "\n",
        "    Args:\n",
        "      logits_raw: Raw logits. Shape: (num_heads, num_tokens, num_tokens).\n",
        "\n",
        "    Returns:\n",
        "      Masked logits. Shape: (num_heads, num_tokens, num_tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    _, num_tokens, _ = logits_raw.shape\n",
        "\n",
        "    # Shape: (num_heads, num_tokens, num_tokens).\n",
        "    attention_mask = compute_attention_mask(num_tokens)\n",
        "    # Shape: (num_heads, num_tokens, num_tokens).\n",
        "    logits_masked = jnp.where(attention_mask, logits_raw, K_MASK)\n",
        "\n",
        "    return logits_masked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MsTEkA1nRfK"
      },
      "source": [
        "### Coding Activity 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPqkFIL-nRfL"
      },
      "outputs": [],
      "source": [
        "def compute_attention_weights(logits_masked: jax.Array) -> jax.Array:\n",
        "    \"\"\"Computes the attention weighs `alpha` from the masked logits by applying\n",
        "    the SoftMax function.\n",
        "\n",
        "    Args:\n",
        "      logits_masked: Masked logits computed in previous steps.\n",
        "        Shape: (num_heads, num_tokens, num_tokens).\n",
        "\n",
        "    Returns:\n",
        "      alpha: The attention weights. Shape: (num_heads, num_tokens, num_tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply the SoftMax.\n",
        "    # Shape: (num_heads, num_tokens, num_tokens).\n",
        "    alpha = jax.nn.softmax(logits_masked)\n",
        "\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lb2HKzLnRqY"
      },
      "source": [
        "### Coding Activity 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyyCqmeLnRqa"
      },
      "outputs": [],
      "source": [
        "def compute_attention_output(\n",
        "    alpha: jax.Array, value_proj: jax.Array\n",
        ") -> jax.Array:\n",
        "    \"\"\"Computes the output of the attention mechanism by computing the weighted\n",
        "    sum of the embeddings in `value_proj`, using the attention weights `alpha`.\n",
        "\n",
        "    Args:\n",
        "      alpha: Attention weights. Shape: (num_heads, num_tokens, num_tokens).\n",
        "      value_proj: Tensor with value projections.\n",
        "        Shape: (num_heads, num_tokens, dim_v).\n",
        "\n",
        "    Returns:\n",
        "      Y: output of the attention mechanism.\n",
        "        Shape: (num_heads, num_tokens, dim_v).\n",
        "    \"\"\"\n",
        "\n",
        "    Y = alpha @ value_proj # Shape: (num_heads, num_tokens, dim_v).\n",
        "\n",
        "    return Y"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8qZfb7Qgzygc",
        "J6ZHKCCKQ95P",
        "a8juDjypQ_HF",
        "InwPXSMynMY2",
        "6E0z2sR4nODQ",
        "_-ov4hLqnOYX",
        "_MsTEkA1nRfK",
        "2Lb2HKzLnRqY"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
