{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DAsej7XSlYt"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3myzQnLMOJ91"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C3-white-bg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE0jaJsaICiX"
      },
      "source": [
        "# Lab: Tune Hyperparameters\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_3/gdm_lab_3_5_tune_hyperparameters.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Optimize hyperparameters to avoid overfitting.\n",
        "\n",
        "15 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSYq3aNrdpnC"
      },
      "source": [
        "## Overview\n",
        "\n",
        "When you train a model, it adjusts its parameters such that it learns the patterns in the training data. However, in order for it to do this most effectively, you, as a model developer, have to find the best settings of the model and the training process. For example, you have to decide how many layers the model should have and for how long the model should be trained. These types of parameters that are configured before a model begins training are called **hyperparameters**. In this lab, you will explore how changing two types of hyperparameters affects how well the model can learn and generalize.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Qf3eGhfl4H"
      },
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will:\n",
        "* Understand how changing the number of layers of a model and changing the number of neurons within a layer affects model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIhcVxx0foVo"
      },
      "source": [
        "### Tasks\n",
        "\n",
        "In this lab, you will again use the dataset from the previous labs in which the classifier aims to predict the next token from the three words \"mat\", \"apple\", and \"bank\". You will then train models with different hyperparameters and investigate their learning behavior thorugh inspecting the learning curves, their accuracy, and their decision boundaries.\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Load the dataset of 2-dimensional embeddings.\n",
        "* Intialize different MLP models.\n",
        "* Train the models and plot the learning curve and decision boundary.\n",
        "\n",
        "All of these steps are described in detail in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNG_jg-39Zj"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in *cells* that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtgZxrpjm6j"
      },
      "source": [
        "Note that the order in which you run the cells matters. When you are working through a lab, make sure to always run all cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-d1hd7Xndke"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will primarily use functions from the custom `ai_foundations` package for training the model and for creating plots.\n",
        "\n",
        "Run the following cell to import all required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhXjPpNZF0BM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import os # For adjusting Keras settings.\n",
        "os.environ['KERAS_BACKEND'] = 'jax' # Set a parameter for Keras.\n",
        "\n",
        "# Packages used.\n",
        "import jax.numpy as jnp # For defining matrices.\n",
        "import pandas as pd # For loading the dataset.\n",
        "import keras # For adjusting Keras settings.\n",
        "from ai_foundations import machine_learning # For defining and training MLPs.\n",
        "from ai_foundations import visualizations # For visualizing data and boundaries.\n",
        "from ai_foundations import training # For logging the loss during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl48p7pc5mEO"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "Run the following cell to load the dataset and prepare the `jax.Arrays` with the training and test data. Note that the data here is manually split into a training set and a test set such that the most challenging examples (all outliers) appear in the test set. This will make it easier to see some of the differences that come from training models with different hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCaHsgjdmjIy"
      },
      "outputs": [],
      "source": [
        "# Load data using pandas.\n",
        "df = pd.read_csv(\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/mat-apple-bank-dataset.csv\")\n",
        "\n",
        "# Extract embeddings (Embedding_dim_1, Embedding_dim_2) and labels.\n",
        "X = jnp.array(df[[\"Embedding_dim_1\", \"Embedding_dim_2\"]].values)\n",
        "# Labels: 0 (\"mat\"), 1 (\"apple\"), or 2 (\"bank\").\n",
        "y = jnp.array(df[\"Label\"].values)\n",
        "\n",
        "# Human-readable labels.\n",
        "labels = [\"mat\", \"apple\", \"bank\"]\n",
        "\n",
        "# Manually split the data.\n",
        "X_train, X_test, y_train, y_test = X[0:61, :], X[61:, :], y[0:61], y[61:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5xQaimmItSD"
      },
      "source": [
        "## Optimizing hyperparameters\n",
        "\n",
        "-----\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        ">As mentioned above, you will experiment here with networks that differ in terms of:\n",
        "> * the number of layers\n",
        "> * the dimensions of each layer (how many neurons there are in each layer)\n",
        ">\n",
        ">In general, the more layers you add and the larger the hidden layer dimensions are, the more likely the model will overfit.\n",
        ">\n",
        ">On the other hand, if you use very few hidden layers (e.g., just one) with a very low dimension (e.g., just 2 neurons), the model will likely not learn a good decision function and underfit.\n",
        ">\n",
        ">Importantly, you will keep everything but the number of layers and the dimensions of the layers constant in this exercise. For example, the number of epochs will be the same across all runs. This allows you to directly attribute any differences in model performance to the hyperparameters that you change.\n",
        ">\n",
        ">Use the cells below to define different network structures by changing `hidden_dims`.\n",
        ">\n",
        ">Then:\n",
        ">1. Visualize the structure of the network to get a sense of what it looks like.\n",
        ">2. Train the network on the dataset.\n",
        ">3. Visualize the decision boundary and observe the train and test accuracies. Recall that the accuracy is defined as the percentage of examples for which the model makes the correct predictions. The train accuracy indicates whether the model learned something â€” low train accuracy suggests **underfitting**. The test accuracy indicates whether the model is able to generalize â€” low test accuracy in combination with high train accuracy suggests **overfitting**.\n",
        ">\n",
        ">\n",
        ">Repeat these three steps with at least three network configurations, that is three different settings of `hidden_dims`.\n",
        ">\n",
        ">Make sure to train:\n",
        ">1. one network with only one layer and a very low number of neurons\n",
        ">2. one network that has at least three layers and the first layer has more than 1,000 neurons\n",
        ">3. one network with settings somewhere in between\n",
        ">\n",
        ">Which ones of these seem to suffer from overfitting? Which ones from underfitting? Can you find settings for which the model seems to generalize well?\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dzHgQgfIsnb"
      },
      "outputs": [],
      "source": [
        "# Set the number of hidden layers and their dimensions here.\n",
        "# Each number indicates the number of neurons per layer. For example, setting\n",
        "# this to `[20, 10]` will initialze an MLP with two hidden layers of dimensions\n",
        "# 20 and 10, respectively.\n",
        "hidden_dims = [20, 10]\n",
        "\n",
        "# Initialize the model and visualize its structure.\n",
        "mlp_model = machine_learning.build_mlp(hidden_dims=hidden_dims, n_classes=3)\n",
        "visualizations.visualize_mlp_architecture(hidden_dims, 3)\n",
        "\n",
        "# Train the model.\n",
        "training_logger = training.CustomAccuracyPrinter(print_every=20)\n",
        "training_history = machine_learning.train_mlp(\n",
        "    mlp_model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    learning_rate=0.005,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=500,\n",
        "    callbacks=[training_logger],\n",
        ")\n",
        "\n",
        "# Plot the loss curve.\n",
        "visualizations.plot_loss_curve(training_history.history)\n",
        "\n",
        "# Compute the accuracy and visualize the decision boundaries.\n",
        "train_acc = training_history.history[\"accuracy\"][-1]\n",
        "test_acc = training_history.history[\"val_accuracy\"][-1]\n",
        "\n",
        "print(f\"Train accuracy: {train_acc * 100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "visualizations.plot_data_and_mlp(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    labels,\n",
        "    features_test=X_test,\n",
        "    label_ids_test=y_test,\n",
        "    mlp_model=mlp_model,\n",
        "    title=\"Decision Boundaries - Your MLP\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78tG3CnTWTFu"
      },
      "source": [
        "### What did you observe?\n",
        "\n",
        "You likely noticed that when you trained a network with a single hidden layer with very few neurons that it had both a low training and test accuracy. This is a classic sign of **underfitting**. The model did not have enough capacity to learn complex enough decision boundaries and therefore was not able to separate the data points.\n",
        "\n",
        "When you trained a network with multiple layers, at least one of which was very large, you probably noticed that the decision boundary was a really complex shape and the test accuracy was lower than the train accuracy (though probably still quite high, since this dataset is not too complex). This indicates **overfitting**. The model has so many parameters that instead of only learning useful patterns from the training data (the signal), it also learned patterns specific to these training examples (the noise). This hindered its ability to generalize to examples in the test set.\n",
        "\n",
        "Finally, when you specified a model with a more reasonable number of parameters (e.g., a layer of 10 neurons and a layer of 5 neurons), you likely observed that it managed to learn a decision boundary that worked well for both the examples in the training set and the examples in the held-out test set. A model such as this is better able to generalize.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtPzT0GxHDvI"
      },
      "source": [
        "## Summary\n",
        "In this activity, you explored how the size of your neural network affects learning.\n",
        "\n",
        "You changed two hyperparameters:\n",
        "\n",
        "* The number of hidden layers\n",
        "* The number of neurons in each layer\n",
        "\n",
        "By testing different model sizes, you saw three possible outcomes:\n",
        "\n",
        "- Underfitting: Small models could not learn enough patterns. Both training and test loss stayed high.\n",
        "- Overfitting: Very large models learned patterns from the training data perfectly, but did not perform as well on the test data and they did not generalize well.\n",
        "- Good: Medium-sized models learned well and worked well on new data.\n",
        "\n",
        "You also visualized:\n",
        "- Loss curves to track how the model learned over time.\n",
        "- Decision boundaries to see how the model separates different classes.\n",
        "\n",
        "In the next activity, you will learn more about additional techniques to mitigate overfitting."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NDWsJUGcf4Ru"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
