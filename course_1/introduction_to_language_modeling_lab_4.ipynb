{
  "cells": [
    {
      "metadata": {
        "id": "6kkanWKTSzW4"
      },
      "cell_type": "markdown",
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "metadata": {
        "id": "keR5bAwlrIo4"
      },
      "cell_type": "markdown",
      "source": [
        "# **Build Your Own Small Language Model, Lab 4: Are You Ready to Build Your Own Small Language Model?**\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/introduction_to_language_modeling_lab_4.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This lab guides you through preparing a text dataset for training a small language model (SLM). The lab focuses on **pre-processing steps** such as tokenization, vocabulary creation, handling out-of-vocabulary words, and adding a padding token to the vocabulary for later use. The lab concludes with the implementation of a simple word tokenizer class.\n"
      ],
      "metadata": {
        "id": "8WbdzNNC7Jrx"
      }
    },
    {
      "metadata": {
        "id": "sYqG7iVwXEX0"
      },
      "cell_type": "code",
      "source": [
        "# Packages used.\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "aS8lTu4FDclK"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load the dataset\n",
        "\n",
        "To begin, you will load the dataset. This lab uses the [Africa Galore](https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json) dataset."
      ]
    },
    {
      "metadata": {
        "id": "x8rRtd8p2DQ6"
      },
      "cell_type": "code",
      "source": [
        "africa_galore = pd.read_json('https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json')\n",
        "train_dataset = africa_galore['description']\n",
        "print(train_dataset.shape)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "YF-eRMqNsQp3"
      },
      "cell_type": "markdown",
      "source": [
        "Now, read  the first paragraph:"
      ]
    },
    {
      "metadata": {
        "id": "WMZ9f1lJOMmF"
      },
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cueGiylBS5Hy"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Convert the text sequence to tokens\n",
        "\n",
        "This step will focus on *tokenization*, which is the process of converting sentences into smaller, manageable units known as *tokens*. You will be using the same function `split_text` developed in the second lab. This function splits words on whitespace so when you tokenize the sentence `'Bimpe didn't come home yesterday'`, you'll get a list of tokens like `['Bimpe', \"didn't\", 'come', 'home', 'yesterday']`. This type of tokenization is called word-level tokenization, because the tokens are words. It is one of the simplest forms of tokenization, as it treats each word as an individual unit of meaning:"
      ]
    },
    {
      "metadata": {
        "id": "JjaimYtAx4j9"
      },
      "cell_type": "code",
      "source": [
        "def split_text(text: str) -> list[str]:\n",
        "    \"\"\"Splits a string into a list of words as tokens.\n",
        "\n",
        "    Splits sentence on whitespace.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "\n",
        "    Returns:\n",
        "        A list of words as tokens. Returns empty list if text is\n",
        "        empty or all whitespace.\n",
        "    \"\"\"\n",
        "    tokens = text.split(' ')\n",
        "    return tokens\n",
        "\n",
        "print(split_text('Here\\'s how you tokenize! Awesome, isn\\'t it?'))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "zid18x_Atf59"
      },
      "cell_type": "markdown",
      "source": [
        "This tokenizer is extremely naive. When splitting on space, you can see how punctuation marks are now part of the token (for example, `'tokenize!'` will be a different token from `'tokenize'`)."
      ]
    },
    {
      "metadata": {
        "id": "oqGWDrXokzbj"
      },
      "cell_type": "code",
      "source": [
        "tokens = [token for paragraph in train_dataset for token in split_text(paragraph)]\n",
        "print('Total number of tokens (words) in our train dataset:', len(tokens))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "_Jt1OWe9tY6n"
      },
      "cell_type": "markdown",
      "source": [
        "Print out the first 20 tokens:"
      ]
    },
    {
      "metadata": {
        "id": "Pdr8290eEum9"
      },
      "cell_type": "code",
      "source": [
        "tokens[:20]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "OWRQLYO7hhp4"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create a vocabulary comprising of unique tokens\n",
        "\n",
        "The *vocabulary* is the set of unique tokens that the model is trained on. These are the building blocks the model uses to understand and generate text data."
      ]
    },
    {
      "metadata": {
        "id": "FbYDEE-Hymhk"
      },
      "cell_type": "markdown",
      "source": [
        "In the cell below, write a function to build the vocabulary (i.e., the list of unique tokens). After you've attempted the function, run the \"Run this to test your code\" cell to check if your solution works. A solution is provided in the third cell, but try to write your own first before reviewing it:"
      ]
    },
    {
      "metadata": {
        "id": "ueYSAjGzj2jp"
      },
      "cell_type": "code",
      "source": [
        "def build_vocab(tokens: list[str])-> list[str]:\n",
        "    # Your code here.\n",
        "    # Build a vocabulary list from the set of tokens.\n",
        "    vocab = ... # Enter code here.\n",
        "    return vocab"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "q9Fk7rH5kF58",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "# @title Run this to test your code.\n",
        "def test_build_vocab():\n",
        "    hint = \"\"\"\n",
        "           Hints:\n",
        "           ======\n",
        "            1. Create a unique set of tokens e.g, if you have\n",
        "               ['hello', 'world', 'world'], it becomes {'hello', 'world'}.\n",
        "               There is a Python `set` function you can use.\n",
        "            2. Convert the set to list e.g {'hello', 'world'} becomes\n",
        "               ['hello', 'world']. There is a Python `list` function that\n",
        "               you can use.\n",
        "          \"\"\"\n",
        "\n",
        "    if build_vocab(['hello', 'world', 'world']) == ['hello', 'world']:\n",
        "        print('Nice! Your answer looks correct.')\n",
        "    elif type(build_vocab(['hello', 'world', 'world'])) == set:\n",
        "        print('\\033[1m\\033[91mSorry, your answer is not correct. Make sure ' +\n",
        "              'that you return a list, not a set.\\033[0m')\n",
        "        give_hints = input('Would you like some hints? Type Yes or No.')\n",
        "        if give_hints.lower() in ['yes', 'y']:\n",
        "            print(hint)\n",
        "    else:\n",
        "        print('\\033[1m\\033[91mSorry, your answer is not correct.\\033[0m')\n",
        "        give_hints = input('Would you like some hints? Type Yes or No.')\n",
        "        if give_hints.lower() in ['yes', 'y']:\n",
        "            print(hint)\n",
        "\n",
        "test_build_vocab()\n",
        "assert build_vocab(['hello', 'world', 'world']) == ['hello', 'world'], '`build_vocab` function is not implemented correctly. Try again.'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ZeJlbyiBjla4",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "# @title build_vocab function solution. Attempt it yourself before revealing the solution\n",
        "\n",
        "def build_vocab(tokens: list[str])-> list[str]:\n",
        "\n",
        "    # Enter code here.\n",
        "\n",
        "    # Build a vocabulary list from the set of tokens.\n",
        "    vocab = list(set(tokens))\n",
        "    return vocab"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2RXLjfFozd9a"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to create the vocabulary `vocab`, and count the number of unique tokens in `vocab`:"
      ]
    },
    {
      "metadata": {
        "id": "t-mULo_viTXK"
      },
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(tokens)\n",
        "\n",
        "# Size of the vocabulary (number of unique tokens).\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(vocab_size)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "LX9mlGBEzpjz"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to print out the first ten tokens in the `vocab`:"
      ]
    },
    {
      "metadata": {
        "id": "YQFWt9KQGN6i"
      },
      "cell_type": "code",
      "source": [
        "vocab[:10]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "_BMsImtfunNb"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4: Add a special \\<UNK\\> token to the vocabulary to handle unknown tokens\n",
        "\n",
        "When you use a model for generation, it is possible for it to encounter unknown tokens. These are patterns that didn't appear in the training data, such as spelling errors. To handle this, you can use the unknown token `<UNK>`. When the model encounters a token not in the vocabulary, it replaces it with `<UNK>`. This prevents the model failing when encountering unknown tokens.\n",
        "\n",
        "For example, if your vocabulary consists of tokens `['Bimpe', 'rode', 'a', 'car', 'yesterday']` and you prompted the model with the phrase `'Bimpe rode a keke-marwa yesterday'`, it will substitute `'keke-marwa' `with `'<UNK>'` because it is an unknown token that does not exist in its vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "-qj05jdIKaFd"
      },
      "cell_type": "markdown",
      "source": [
        "You can *append* (add to the end) the unknown token `'<UNK>'` to the vocabulary:"
      ]
    },
    {
      "metadata": {
        "id": "4nzQiIV0vAwe"
      },
      "cell_type": "code",
      "source": [
        "UNKNOWN_TOKEN = '<UNK>'\n",
        "\n",
        "vocab.append(UNKNOWN_TOKEN)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(vocab_size)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "gz9VRoR8bqSA"
      },
      "cell_type": "markdown",
      "source": [
        "The size of the vocabulary has increased by 1.\n",
        "\n",
        "Print out the last ten tokens in the vocabulary:"
      ]
    },
    {
      "metadata": {
        "id": "vq1uKR8pbyHj"
      },
      "cell_type": "code",
      "source": [
        "vocab[-10:]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ej4MUpn_b3lp"
      },
      "cell_type": "markdown",
      "source": [
        "You now see the special unknown token `'<UNK>'` at the very bottom."
      ]
    },
    {
      "metadata": {
        "id": "-Z6vtwjGLQcH"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 5: Convert the tokens into token IDs (or indexes)\n",
        "\n",
        "It is convenient to represent the tokens as numbers (or indexes), where each token is represented by a number between `0` and `vocab_size`.\n",
        "\n",
        "There are two dictionaries to facilitate this mapping:\n",
        "\n",
        "1. **`index_to_token`**: This dictionary maps an index (a number) back to its corresponding token (a string). Given an index between 0 and the vocabulary size, it returns the token at that position.\n",
        "2. **`token_to_index`**: This dictionary maps each token in the vocabulary to its corresponding index.\n",
        "\n",
        "Now, when you need to convert a token to a number, use `token_to_index`. And when you need to convert a number back to a token, use `index_to_token`."
      ]
    },
    {
      "metadata": {
        "id": "7dPpiZnq0dbx"
      },
      "cell_type": "markdown",
      "source": [
        "Next, create a dictionary below called `index_to_token`, where the index is the key and the token is the value. This dictionary should be the reverse of the `token_to_index` dictionary. After implementing the dictionary, run the cell and verify that the tokens and their corresponding indexes match between `token_to_index` and `index_to_token`:"
      ]
    },
    {
      "metadata": {
        "id": "ejO4vHqkjBWd"
      },
      "cell_type": "code",
      "source": [
        "# Note the index here is starting with 1.\n",
        "# The first index is reserved for another special <PAD> token, explained below.\n",
        "token_to_index = {token: index+1 for index, token in enumerate(vocab)}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "-cx5cO--6mnw"
      },
      "cell_type": "code",
      "source": [
        "# Create a dictionary that maps an index (a number) back to\n",
        "# its corresponding token in the vocab, starting the index with 1.\n",
        "index_to_token = ... # Enter code here."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "9VKsd6ILx8Dc"
      },
      "cell_type": "markdown",
      "source": [
        "Get the index for the special unknown `'<UNK>'` token:"
      ]
    },
    {
      "metadata": {
        "id": "F3_Xr9tQkx7I"
      },
      "cell_type": "code",
      "source": [
        "token_to_index[UNKNOWN_TOKEN]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "OsGR3AsM3oqc"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to test your `index_to_token` function to make sure it is correct. It should return `True` if your implementation is correct:"
      ]
    },
    {
      "metadata": {
        "id": "1r_u65kmDMH_"
      },
      "cell_type": "code",
      "source": [
        "print(index_to_token[token_to_index[UNKNOWN_TOKEN]] == UNKNOWN_TOKEN)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Xan-_ZYxtUpk",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "# @title index_to_token solution. Attempt it yourself before revealing the solution\n",
        "index_to_token = {index+1: token for index, token in enumerate(vocab)}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fFMHj6_lEiYy"
      },
      "cell_type": "markdown",
      "source": [
        "You have now completed the indexing step. Now, examine the first ten tokens and their indexes:"
      ]
    },
    {
      "metadata": {
        "id": "yMYC96xx2Szz"
      },
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for key, value in token_to_index.items():\n",
        "    # `repr` returns a printable representational string.\n",
        "    print(f'{repr(key)}: {value}')\n",
        "    count += 1\n",
        "    if count == 10:\n",
        "        break"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "p6tir27_FyqL"
      },
      "cell_type": "markdown",
      "source": [
        "Next, examine the first ten indexes and their tokens:"
      ]
    },
    {
      "metadata": {
        "id": "oJLkxuJU1S4v"
      },
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for key, value in index_to_token.items():\n",
        "    # `repr` returns a printable representational string.\n",
        "    print(f'{key}: {repr(value)}')\n",
        "    count += 1\n",
        "    if count == 10:\n",
        "        break"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "EGpnUe8vF25C"
      },
      "cell_type": "markdown",
      "source": [
        "Notice how `token_to_index` and `index_to_token` are inverses of each other:"
      ]
    },
    {
      "metadata": {
        "id": "CSo1a6ow0qVN"
      },
      "cell_type": "markdown",
      "source": [
        "Find the index of the token `'fans'`:"
      ]
    },
    {
      "metadata": {
        "id": "6L00m3Ik1dAE"
      },
      "cell_type": "code",
      "source": [
        "token_to_index['fans']"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "68U0m26vzDpU"
      },
      "cell_type": "markdown",
      "source": [
        "Find the index of the unknown token:"
      ]
    },
    {
      "metadata": {
        "id": "KHCQOf8-3Bin"
      },
      "cell_type": "code",
      "source": [
        "token_to_index[UNKNOWN_TOKEN]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5o76AGHNyRo1"
      },
      "cell_type": "markdown",
      "source": [
        "Find the index of the token `'photosynthesis'`:"
      ]
    },
    {
      "metadata": {
        "id": "cAGf7Ig21_qH"
      },
      "cell_type": "code",
      "source": [
        "token_to_index['photosynthesis']"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "8bJgwgrT2j9N"
      },
      "cell_type": "markdown",
      "source": [
        "`'photosynthesis'` does not exist in the dataset hence `token_to_index`, which explains how it finds its index. As discussed above, in such cases, you want to return the index of the `'<UNK>'` token. You can use the [.get(key, value)](https://www.w3schools.com/python/ref_dictionary_get.asp) dictionary method, which allows a default `value` when the `key` is not in the dictionary."
      ]
    },
    {
      "metadata": {
        "id": "Yu-ze5yFaOOd"
      },
      "cell_type": "markdown",
      "source": [
        "Here you can see the unknown token index being returned for a word that does not exist in the vocabulary:"
      ]
    },
    {
      "metadata": {
        "id": "zWZnCPZR1BwX"
      },
      "cell_type": "code",
      "source": [
        "token_to_index.get('photosynthesis', token_to_index[UNKNOWN_TOKEN])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1IiZ7XVIG1P-"
      },
      "cell_type": "markdown",
      "source": [
        "The 5451 is the token ID for `'<UNK>'` token. Run the cell below to verify:"
      ]
    },
    {
      "metadata": {
        "id": "F74fomp3GunK"
      },
      "cell_type": "code",
      "source": [
        "index_to_token[5451]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "VgOkb5VSwnbm"
      },
      "cell_type": "markdown",
      "source": [
        "**The encode and decode functions**\n",
        "\n",
        "Create two functions: `encode` and `decode`.\n",
        "- The `encode` function takes a string of text and returns corresponding indices of the tokens. Whenever it encounters a token not in the vocab, it will return the index of the `'<UNK>'` token.\n",
        "- The `decode` function takes a list of indices and returns the text associated with it.\n"
      ]
    },
    {
      "metadata": {
        "id": "JC_dnmc2xjBP"
      },
      "cell_type": "code",
      "source": [
        "def encode(text: str) -> list[int]:\n",
        "    return [token_to_index.get(token, token_to_index[UNKNOWN_TOKEN])\n",
        "            for token in split_text(text)]\n",
        "\n",
        "\n",
        "def decode(numbers: int | list[int]) -> list[str]:\n",
        "    return [index_to_token.get(number, UNKNOWN_TOKEN) for number in numbers]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "GXnEuuHPH-Rx"
      },
      "cell_type": "markdown",
      "source": [
        "The next step is to verify and encode the string of text into indexes, and then decode those indexes. You should get the original text back. Now read the first paragraph of the train dataset:"
      ]
    },
    {
      "metadata": {
        "id": "84E5f-ME_0Fb"
      },
      "cell_type": "code",
      "source": [
        "text = train_dataset[0]\n",
        "print(text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7FsCmasazTgc"
      },
      "cell_type": "markdown",
      "source": [
        "Encode the text and look at the first ten tokens:"
      ]
    },
    {
      "metadata": {
        "id": "6K2wAHCKHknD"
      },
      "cell_type": "code",
      "source": [
        "encode(text)[:10]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "IdE_qUfezXSo"
      },
      "cell_type": "markdown",
      "source": [
        "Decode the encoded text back and check the expected outcome:\n"
      ]
    },
    {
      "metadata": {
        "id": "y65u6LNEHigD"
      },
      "cell_type": "code",
      "source": [
        "decode(encode(text)[:10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bPdM0SkpIWbw"
      },
      "cell_type": "markdown",
      "source": [
        "Now encode all paragraphs in the train dataset:"
      ]
    },
    {
      "metadata": {
        "id": "eP21m4ENISex"
      },
      "cell_type": "code",
      "source": [
        "encoded_tokens = [encode(text) for text in train_dataset]\n",
        "len(encoded_tokens)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Ub5kMzgQs-B5"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to convert the list of `encoded_tokens` to a numerical array.\n",
        "\n",
        "Neural networks operate on and process numerical arrays.\n",
        "\n",
        "> **NOTE**: Another word for a multi-dimensional numerical array is a *tensor*."
      ]
    },
    {
      "metadata": {
        "id": "UEUXl_NlsO0T"
      },
      "cell_type": "code",
      "source": [
        "encoded_tensor = tf.convert_to_tensor(encoded_tokens, dtype=tf.int32)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "uOS2e4YAuC9i"
      },
      "cell_type": "markdown",
      "source": [
        "What happened?\n",
        "\n",
        "The cell above gave an error: `ValueError: can't convert non-rectangular Python sequence to Tensor`. But what does that mean?\n",
        "\n",
        "This error happens because TensorFlow requires all the lists (or sequences) you're trying to convert into a tensor to have the same size. In other words, the lists must form a *rectangular shape*, as all the inner lists need to have the same number of elements.\n",
        "\n",
        "Now look at this example: `[[1, 2, 3], [1, 2]]`.\n",
        "\n",
        "If you try to turn this into a tensor, it will give an error. This is because the two inner lists have different lengths. The first list has three numbers, but the second one has only two.\n",
        "\n",
        "To fix this, you need to make sure all the lists have the same length. This can be done in two ways:\n",
        "\n",
        "1. **Padding**: You can add extra values (like `0` or `-1`) to the shorter list, making it the same length as the longer one. For example: `[[1, 2, 3], [1, 2, 0]]`.\n",
        "\n",
        "   `0` was added to the second list as a placeholder. This way, both lists have three elements.\n",
        "\n",
        "2. **Truncation**: You can shorten the longer list to match the shorter one. For example: `[[1, 2], [1, 2]]`.\n",
        "\n",
        "   In this case, the extra element was removed from the first list, so both lists now have two elements.\n",
        "\n",
        "Next, add padding to make sure all the sequences in the train dataset have the same length. This will allow you to turn them into a proper tensor."
      ]
    },
    {
      "metadata": {
        "id": "X8zWgAENq1SV"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 6: Add the special padding token to handle varying input lengths\n",
        "\n",
        "The *padding* process ensures that sequences of varying lengths are all the same size. This is done by adding the special token `'<PAD>'` to shorter sequences. This way they align with the longest sequence in the dataset. Padding is necessary to create consistent input for the model (input of the same shape).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JblgHC9d7KM-"
      },
      "cell_type": "markdown",
      "source": [
        "Before you can pad the token, you first need to update our vocabulary to include the special pad token `'<PAD>'`.\n",
        "\n",
        "To do this, you will insert the `'<PAD>'` token (add to the beginning) of the vocabulary list. This way, the index for `'<PAD>'` will be 0, making it the first token in the vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "Id1v9y4a5q5F"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to add the pad token to the beginning of the `vocab` list:"
      ]
    },
    {
      "metadata": {
        "id": "iNlH82HZChfA"
      },
      "cell_type": "code",
      "source": [
        "PAD_TOKEN = '<PAD>'\n",
        "# Inserts pad token in the first position of the list.\n",
        "vocab.insert(0, PAD_TOKEN)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(vocab_size)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "85njWfBi5nDr"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to print out the first ten tokens:"
      ]
    },
    {
      "metadata": {
        "id": "AHerURefC3Ln"
      },
      "cell_type": "code",
      "source": [
        "vocab[:10]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "E1hjNeAx6G3a"
      },
      "cell_type": "markdown",
      "source": [
        "Remember, you started the `token_to_index` and `index_to_token` dictionaries with the first index. You left the 0th index free  so that the `'<PAD>'` token can be added there. Now, added the following:"
      ]
    },
    {
      "metadata": {
        "id": "nGpfFrWfC8S3"
      },
      "cell_type": "code",
      "source": [
        "token_to_index['<PAD>'] = 0\n",
        "index_to_token[0] = '<PAD>'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7Z1NRV5K3t-i"
      },
      "cell_type": "code",
      "source": [
        "index_to_token[0]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "kllG8M7e4UjH"
      },
      "cell_type": "markdown",
      "source": [
        "Before you can proceed with padding the sequences, it's useful to create a handy Python class that will bring together everything learned so far about tokenizing text and converting it into numbers.\n",
        "\n",
        "Call this class `SimpleWordTokenizer`. This class will organize the code and help manage the vocabulary, and the `token-to-index` and `index-to-token` dictionaries. It will also perform tokenization, encoding, and decoding as you have done above. There's nothing new here. It is simply taking all the concepts you've learned and organizing them into a class structure to make the code cleaner and more efficient.\n",
        "\n",
        "This `SimpleWordTokenizer` class provides a solid foundation for understanding tokenization methods used in language modeling. As you continue to explore the world of language modeling further, you'll come across other tokenization techniques that follow a similar structure. By building this class now, you'll have a strong base to build upon:"
      ]
    },
    {
      "metadata": {
        "id": "nPk5t3kUwpOy"
      },
      "cell_type": "code",
      "source": [
        "# Putting it all together.\n",
        "class SimpleWordTokenizer:\n",
        "    \"\"\"A simple word tokenizer that can be initialized with texts\n",
        "    or using a provided vocabulary list.\n",
        "\n",
        "    The tokenizer splits text sequences based on whitespace, using the `encode`\n",
        "    method to convert text into a sequence of indices and the `decode` method\n",
        "    to convert indices back into text. It handles unknown words and includes\n",
        "    padding functionality.\n",
        "\n",
        "    Typical usage example:\n",
        "\n",
        "        text = 'Hello there!'\n",
        "        tokenizer = SimpleWordTokenizer(text)\n",
        "        print(tokenizer.encode('Hello'))\n",
        "\n",
        "    Attributes:\n",
        "        UNKNOWN_TOKEN: A string constant representing the special\n",
        "            token for unknown words.\n",
        "        PAD_TOKEN: A string constant representing the special token\n",
        "            for padding.\n",
        "        texts: Input text dataset used to build the vocabulary if no 'vocab' is\n",
        "            provided.\n",
        "        vocab: A pre-defined vocabulary. Defaults to None. If None,\n",
        "            the vocabulary is automatically inferred from the texts.\n",
        "            The inferred vocabulary includes PAD_TOKEN at index 0\n",
        "            and UNKNOWN_TOKEN at the last index.\n",
        "        vocab_size: The total number of tokens in the vocabulary,\n",
        "            including special tokens.\n",
        "        token_to_index: A dictionary mapping tokens to their corresponding\n",
        "            indices.\n",
        "        index_to_token: A dictionary mapping indices to their corresponding\n",
        "            tokens.\n",
        "        pad_token_id: The index of the PAD_TOKEN in the vocabulary.\n",
        "        unknown_token_id: The index of the UNKNOWN_TOKEN.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define constants.\n",
        "    UNKNOWN_TOKEN: str = '<UNK>'\n",
        "    PAD_TOKEN: str = '<PAD>'\n",
        "\n",
        "\n",
        "    def __init__(self, texts: list[str], vocab: list[str] | None = None):\n",
        "        \"\"\"Initializes the tokenizer with texts or using a provided vocabulary.\n",
        "\n",
        "        Args:\n",
        "          texts: Input text dataset.\n",
        "          vocab: A pre-defined vocabulary. Defaults to None. If None,\n",
        "                the vocab is automatically inferred from the texts.\n",
        "        \"\"\"\n",
        "\n",
        "        if vocab is None:\n",
        "            # Build the vocab from scratch.\n",
        "            if isinstance(texts, str):\n",
        "                texts = [texts]\n",
        "\n",
        "            # Step 2: Convert text sequence to tokens.\n",
        "            tokens = [token for text in texts\n",
        "                      for token in self.split_text(text)]\n",
        "\n",
        "            # Step 3: Create a vocabulary comprising of unique tokens.\n",
        "            vocab = self.build_vocab(tokens)\n",
        "\n",
        "            # Step 4 and 6: Add special unknown and pad token to the vocabulary.\n",
        "            self.vocab = [self.PAD_TOKEN] + vocab +  [self.UNKNOWN_TOKEN]\n",
        "\n",
        "        else:\n",
        "          self.vocab = vocab\n",
        "\n",
        "        # Size of vocabulary.\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        # Create token-to-index and index-to-token mappings.\n",
        "        self.token_to_index = {token: index\n",
        "                               for index, token in enumerate(self.vocab)}\n",
        "        self.index_to_token = {index: token\n",
        "                               for index, token in enumerate(self.vocab)}\n",
        "\n",
        "        # Map the special tokens to their IDs.\n",
        "        self.pad_token_id = self.token_to_index[self.PAD_TOKEN]\n",
        "        self.unknown_token_id = self.token_to_index[self.UNKNOWN_TOKEN]\n",
        "\n",
        "\n",
        "    def split_text(self, text: str) -> list[str]:\n",
        "        \"\"\"Splits a given text on whitespace into tokens (tokens).\"\"\"\n",
        "        return text.split(' ')\n",
        "\n",
        "\n",
        "    def join_text(self, text_lists: list[str]) -> str:\n",
        "        \"\"\"Combines a list of tokens into a single string,\n",
        "            with tokens separated by spaces.\n",
        "        \"\"\"\n",
        "        return ' '.join(text_lists)\n",
        "\n",
        "\n",
        "    def build_vocab(self, tokens: list[str])-> list[str]:\n",
        "      \"\"\"Create a vocabulary list from the set of tokens\"\"\"\n",
        "      return list(set(tokens))\n",
        "\n",
        "\n",
        "    # This is the same function from step 5.\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        \"\"\"Encodes a text sequence into a list of indices based on the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            text: The input text to be encoded.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of indices corresponding to the tokens in the\n",
        "                  input text.\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 5: Convert tokens into indexes.\n",
        "        return [self.token_to_index.get(token,\n",
        "                                        self.token_to_index[self.UNKNOWN_TOKEN])\n",
        "                for token in self.split_text(text)]\n",
        "\n",
        "\n",
        "    # This is mostly the same function that was developed in step 5.\n",
        "    def decode(self, numbers: int | list[int]) -> str:\n",
        "        \"\"\"Decodes a list (or single index) of integers back into\n",
        "        corresponding tokens from the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            numbers: A single index or a list of indices to be\n",
        "                     decoded into tokens.\n",
        "\n",
        "        Returns:\n",
        "            str: A string of decoded tokens corresponding to the input indices.\n",
        "        \"\"\"\n",
        "\n",
        "        # If a single integer is passed, convert it into a list.\n",
        "        if isinstance(numbers, int):\n",
        "            numbers = [numbers]\n",
        "\n",
        "        # Map indices to tokens.\n",
        "        tokens = [self.index_to_token.get(number, self.unknown_token_id) for number in numbers]\n",
        "\n",
        "        # Join the decoded tokens into a single string.\n",
        "        return self.join_text(tokens)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "mgV0fzQB5LrN"
      },
      "cell_type": "markdown",
      "source": [
        "Verify that the class created returns the same vocabulary as the one made before:"
      ]
    },
    {
      "metadata": {
        "id": "0rMKmNlRY-3q"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleWordTokenizer(train_dataset)\n",
        "assert tokenizer.vocab == vocab\n",
        "assert tokenizer.decode(tokenizer.encode(train_dataset[0])) == train_dataset[0]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "R2hm5-p17TdK"
      },
      "cell_type": "markdown",
      "source": [
        "In the cell above, you can run some tests using `assert` statements to make sure that the first paragraph from the training dataset remains the same after encoding and then decoding it. You can also inspect this yourself to check that everything works as expected.\n",
        "\n",
        "Run the cell below to check the first paragraph in the training dataset. Apply the `tokenizer.encode` method from the tokenizer class to convert it into numbers. Then, use the `tokenizer.decode` method to convert it back into text. Finally, compare the decoded text with the original paragraph to ensure they match:"
      ]
    },
    {
      "metadata": {
        "id": "HjS_GGvXcvlu"
      },
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(train_dataset[0]))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "X5iUX5Jx8JJT"
      },
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "XTpLL64I5P2g"
      },
      "cell_type": "markdown",
      "source": [
        "Now you can simply use the tokenizer to encode the text data. It will perform steps 2-6 above:"
      ]
    },
    {
      "metadata": {
        "id": "ukMJSFQai7IZ"
      },
      "cell_type": "code",
      "source": [
        "encoded_tokens = [tokenizer.encode(text) for text in train_dataset]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "sZdmYZ0Bqfn9"
      },
      "cell_type": "markdown",
      "source": [
        "## Reflection\n",
        "\n",
        "This is the end of **Lab 4: Are You Ready to Build your Own Small Language Model?**\n",
        "\n",
        "This lab guided you through preparing a text dataset for training a small language model (SLM), focusing on:\n",
        "\n",
        "- **Loading and exploring the dataset:** You examined the structure and content of the Africa Galore dataset, focusing on the text descriptions.\n",
        "\n",
        "- **Tokenized the text:** You used a simple word-level tokenization method to split the text into individual words, creating a vocabulary of unique tokens.\n",
        "\n",
        "- **Handled unknown words:** You added a special `'<UNK>'` token to the vocabulary to represent words not encountered during training.\n",
        "\n",
        "- **Created numerical representations:** You mapped each token to a unique numerical index, creating `token_to_index` and `index_to_token` dictionaries to facilitate conversions between words and numbers.\n",
        "\n",
        "- **Addressed varying sequence lengths:** You introduced padding, using the `'<PAD>'` token, to ensure all text sequences have the same length, a requirement for processing data in neural networks.\n",
        "\n",
        "- **Built a tokenizer class:** You consolidated all the tokenization and encoding/decoding logic into a reusable SimpleWordTokenizer class. This class streamlines the process of converting text into numerical data that can be fed into a language model.\n",
        "\n",
        "In the next lab, you will use this tokenizer class to tokenize the data that you will be training a small language model on."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
