{
  "cells": [
    {
      "metadata": {
        "id": "OCsFmIMbStx9"
      },
      "cell_type": "markdown",
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "metadata": {
        "id": "DqkGkF3Dtwqt"
      },
      "cell_type": "markdown",
      "source": [
        "# **Build Your Own Small Language Model, Lab 3: Experiment with a Transformer Model**\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/introduction_to_language_modeling_lab_3.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>"
      ]
    },
    {
      "metadata": {
        "id": "DygES4SNYzHJ"
      },
      "cell_type": "code",
      "source": [
        "# Packages used.\n",
        "from IPython.display import clear_output"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "i_revaKEC9GA"
      },
      "cell_type": "markdown",
      "source": [
        "Run the (hidden) code cell below to load a pre-trained Gemma language model."
      ]
    },
    {
      "metadata": {
        "id": "ExeLaI42XZjU",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "# @title Hidden code used for loading a pre-trained Gemma language model.\n",
        "# The gemma package provides tools for working with Gemma language models,\n",
        "# including loading and prompting.\n",
        "# Install it using `!pip install gemma==3.0.0`.\n",
        "!pip install gemma==3.0.0\n",
        "from IPython.display import clear_output\n",
        "clear_output()  # Clears the output\n",
        "import os\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "from gemma import gm\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "def prompt_transformer_model(input_text: str,\n",
        "                             max_new_tokens: int = 10,\n",
        "                             model_name: int = 'Gemma-1B',\n",
        "                             do_sample: bool = True) -> tuple[str, np.ndarray, Any]:\n",
        "    \"\"\"Generate text from a transformer model (Gemma) based on the input text.\n",
        "\n",
        "    Args:\n",
        "        input_text: The input prompt for the model.\n",
        "        max_new_tokens: The maximum number of new tokens to generate.\n",
        "        model_name: The name of the model to load. Supported options are\n",
        "                    'Gemma-1B' and 'Gemma-4B'. Defaults to 'Gemma-1B'.\n",
        "        do_sample: Whether to use sampling for text generation (True for random\n",
        "                  sampling, False for greedy).\n",
        "\n",
        "    Returns:\n",
        "        output_text: The generated text, including the input text and the\n",
        "                     model's output.\n",
        "        next_token_logits: Logits for the next token (probability distribution).\n",
        "        tokenizer: The tokenizer used for encoding/decoding the text.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the model_name is not recognized or supported.\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(do_sample, bool), 'do_sample must be a boolean value.'\n",
        "\n",
        "    # Process for Gemma-based models.\n",
        "    if model_name not in ['Gemma-1B', 'Gemma-4B']:\n",
        "        raise ValueError(f'model_name=`{model_name}` is not supported. '\n",
        "                        'Supported options are \\'Gemma-1B\\' and \\'Gemma-4B\\'')\n",
        "\n",
        "    tokenizer, model, params = load_gemma(model_name)\n",
        "    sampler = gm.text.Sampler(\n",
        "        model=model,\n",
        "        params=params,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    if not do_sample:\n",
        "        sampler_output_text = sampler.sample(input_text,\n",
        "                                            max_new_tokens=max_new_tokens,\n",
        "                                            sampling=gm.text.Greedy())\n",
        "    else:\n",
        "        sampler_output_text = sampler.sample(input_text,\n",
        "                                            max_new_tokens=max_new_tokens,\n",
        "                                            sampling=gm.text.RandomSampling())\n",
        "\n",
        "    # Convert the input text to tokens and apply the model to generate predictions\n",
        "    prompt = tokenizer.encode(input_text, add_bos=True)\n",
        "    prompt = jnp.asarray(prompt)\n",
        "    out = model.apply(\n",
        "        {'params': params},\n",
        "        tokens=prompt,\n",
        "        return_last_only=True  # Only predict the last token.\n",
        "    )\n",
        "    next_token_logits = out.logits\n",
        "    output_text = input_text + sampler_output_text\n",
        "\n",
        "    return output_text, next_token_logits, tokenizer\n",
        "\n",
        "\n",
        "def load_gemma(model_name: str = 'Gemma-1B') -> tuple:\n",
        "    \"\"\"Loads a Gemma model and its associated tokenizer and parameters.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the Gemma model to load. Options are: 'Gemma-1B'\n",
        "                    and 'Gemma-4B'.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: Tokenizer for the specified Gemma model.\n",
        "        model: The Gemma model.\n",
        "        params: The parameters for the specified Gemma model.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unsupported model name is provided.\n",
        "    \"\"\"\n",
        "    # Set the full GPU memory usage for JAX\n",
        "    os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '1.00'\n",
        "\n",
        "    # Initialize variables\n",
        "    tokenizer = None\n",
        "    model = None\n",
        "    params = None\n",
        "\n",
        "    # Model loading based on model_name\n",
        "    if model_name == 'Gemma-1B':\n",
        "        tokenizer = gm.text.Gemma3Tokenizer()\n",
        "        model = gm.nn.Gemma3_1B()\n",
        "        params = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA3_1B_PT)\n",
        "    elif model_name == 'Gemma-4B':\n",
        "        tokenizer = gm.text.Gemma3Tokenizer()\n",
        "        model = gm.nn.Gemma3_4B()\n",
        "        params = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA3_4B_PT)\n",
        "    else:\n",
        "        raise ValueError(f'Unsupported model name: {model_name}. '\n",
        "                        'Please use \\'Gemma-1B\\' or \\'Gemma-4B\\'.')\n",
        "\n",
        "    return tokenizer, model, params\n",
        "\n",
        "\n",
        "def plot_next_token(probs_or_logits: np.ndarray,\n",
        "                    tokenizer: Any,\n",
        "                    prompt: str,\n",
        "                    keep_top: int = 30):\n",
        "    \"\"\"Plots the probability distribution of the next tokens.\n",
        "\n",
        "    This function generates a bar plot showing the top `keep_top`\n",
        "    tokens by probability.\n",
        "\n",
        "    # Function from gemma\n",
        "    https://github.com/google-deepmind/gemma/blob/ee0d55674ecd0f921d39d22615e4e79bd49fce94/gemma/gm/text/_tokenizer.py#L249-L284\n",
        "\n",
        "    Args:\n",
        "        probs_or_logits: The raw logits output by the model or\n",
        "                          the probability distribution for the next token\n",
        "                          prediction.\n",
        "        tokenizer: The tokenizer used to decode token IDs to human-readable text.\n",
        "        prompt: The input prompt used to generate the next token predictions.\n",
        "        keep_top: The number of top tokens to display in the plot. Default is 30.\n",
        "\n",
        "    Returns:\n",
        "        None: Displays a plot showing the probability distribution of the top\n",
        "              tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    if np.isclose(probs_or_logits.sum(), 1):\n",
        "        probs = probs_or_logits\n",
        "    else:\n",
        "      # Apply softmax to logits to get probabilities\n",
        "        probs = jax.nn.softmax(probs_or_logits)\n",
        "\n",
        "    # Select the top `keep_top` tokens by probability\n",
        "    indices = jnp.argsort(probs)\n",
        "\n",
        "    # Reverse to get highest probabilities first\n",
        "    indices = indices[-keep_top:][::-1]\n",
        "\n",
        "    # Get the probabilities and corresponding tokens\n",
        "    probs = probs[indices].astype(np.float32)\n",
        "    tokens = [repr(tokenizer.decode(i.item())) for i in indices]\n",
        "\n",
        "    # Create the bar plot using Plotly\n",
        "    fig = px.bar(x=tokens, y=probs)\n",
        "\n",
        "    # Customize the plot layout\n",
        "    fig.update_layout(\n",
        "        title=f'Probability Distribution of Next Tokens given the prompt=\"{prompt}\"',\n",
        "        xaxis_title='Tokens',\n",
        "        yaxis_title='Probability',\n",
        "    )\n",
        "\n",
        "    # Display the plot\n",
        "    fig.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "gpCVDaGF95Hj"
      },
      "cell_type": "markdown",
      "source": [
        "## Predicting a next token using a pre-trained Gemma model\n",
        "\n",
        "**Your task**\n",
        "1. Select the transformer model from the `model_name` dropdown menu.\n",
        "2. Enter a prompt of your choice using the `prompt` text field.\n",
        "3. Run the cell.\n",
        "4. Inspect the model's prediction for the next token.\n",
        "\n",
        "For example, if you start with the prompt: `'Jide was hungry so she went looking for'` the transformer model will predict the next token. A token can be a single character (like `'T'`), a full word (like `'The'`), or a subword (such as `'Th'`).\n",
        "\n",
        "Try running the cell several times to observe how the model responds to different prompts:"
      ]
    },
    {
      "metadata": {
        "id": "eIT5solyY3ql"
      },
      "cell_type": "markdown",
      "source": [
        "> Is the cell below running slowly? Change the `model_name` to try out a different model size. Remember, a model with fewer parameters is faster."
      ]
    },
    {
      "metadata": {
        "id": "3wf_FBdgRKmP"
      },
      "cell_type": "code",
      "source": [
        "model_name = 'Gemma-1B' #@param ['Gemma-1B', 'Gemma-4B']\n",
        "\n",
        "prompt = 'Jide was hungry so she went looking for' #@param {type: 'string'}\n",
        "\n",
        "output_text, next_token_logits, tokenizer = prompt_transformer_model(prompt, max_new_tokens=1, model_name=model_name)\n",
        "clear_output() # Clears the output.\n",
        "\n",
        "print(output_text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "-nV9tK-R42Ja"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the probability distribution of the predicted next token"
      ]
    },
    {
      "metadata": {
        "id": "JmG3D2U6wos4"
      },
      "cell_type": "markdown",
      "source": [
        "Now that you've seen the model's prediction, it's important to examine the probability distribution behind the next token. The transformer model calculates the likelihood of each possible next token. This is based on the context (prior words) of the prompt you provided and samples from the probability distribution.\n",
        "\n",
        "The plot below visualizes the probability distribution of the next token predicted by the language model and the prompt.  Each bar represents a different token, and its height corresponds to the probability assigned to that token by the model.  \n",
        "\n",
        "Visualizing the probability distribution allows you to analyze the model's preferences for different token choices after being given a prompt.  A highly peaked distribution suggests high confidence in a single prediction, while a flatter distribution indicates greater uncertainty and a broader range of plausible next tokens.\n",
        "\n",
        "Run the cell below:"
      ]
    },
    {
      "metadata": {
        "id": "iNbt0z7AwnzL"
      },
      "cell_type": "code",
      "source": [
        "plot_next_token(next_token_logits, tokenizer, prompt=prompt)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fH5is1_RLnuS"
      },
      "cell_type": "markdown",
      "source": [
        "When you run the cell above, the model generates a probability distribution for the next token. Some tokens will have higher probabilities than others, meaning they are more likely to be chosen as the next word."
      ]
    },
    {
      "metadata": {
        "id": "lIr08bp1ZtVo"
      },
      "cell_type": "markdown",
      "source": [
        "Here are a few likely observations using the `Gemma-1B` model:\n",
        "\n",
        "1. The most probable token will usually be a common word that fits the context of the sentence (e.g., `'food'` after the prompt `'Jide was hungry so she went looking for'`).\n",
        "2. The model might suggest words that seem plausible but aren't always the most expected, like `'a'` or `'something'`.\n",
        "3. You might notice some tokens have low probabilities, meaning the model considers them less likely to fit but doesn't completely rule them out, like `'work'` or `'help'`.\n",
        "4. Changing the transformer model may result in slight variations in the predicted next token. This is because the prediction is influenced by the model's parameters, which are determined by the dataset used for training.\n",
        "\n",
        "Try out different prompts and observe the probability distribution of the next token prediction."
      ]
    },
    {
      "metadata": {
        "id": "oy9BsjpKLuhE"
      },
      "cell_type": "markdown",
      "source": [
        "### Changing the context slightly\n",
        "\n",
        "What happens to the probability distribution if the context is changed? Try `'Jide was thirsty so she went looking for'`:"
      ]
    },
    {
      "metadata": {
        "id": "TFCkKEfAMFG-"
      },
      "cell_type": "code",
      "source": [
        "model_name = 'Gemma-1B' #@param ['Gemma-1B', 'Gemma-4B']\n",
        "\n",
        "prompt = 'Jide was thirsty so she went looking for' #@param {type: 'string'}\n",
        "\n",
        "output_text, next_token_logits, tokenizer = prompt_transformer_model(prompt,\n",
        "                                                                     max_new_tokens=1,\n",
        "                                                                     model_name=model_name)\n",
        "clear_output() # Clears the output.\n",
        "\n",
        "plot_next_token(next_token_logits, tokenizer, prompt=prompt)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "OxL_TBTMTI8u"
      },
      "cell_type": "markdown",
      "source": [
        "#### What did you observe?\n",
        "\n",
        "When running the transformer model with prompts like `'Jide was thirsty so she went looking for'`, you might notice certain patterns in the predicted next tokens. For instance, you may see drink-related words like \"water\" suggested more often. This is because the transformer model is **context-aware** and understands that terms related to hunger and thirst tend to align with certain words—like `'food'` or `'water'`—based on the context provided by the prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparison between transformer models\n",
        "\n",
        "Different transformer models can sometimes generate different next tokens, even for the same prompt. You might see variations in the suggestions depending on the size and training of the model you're using. Larger models, with more data and parameters, tend to generate more accurate and contextually appropriate predictions. Smaller models might be more limited in their understanding, occasionally offering less relevant or more generic predictions."
      ],
      "metadata": {
        "id": "Vqbk-1LzAi67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer models versus n-gram models\n",
        "\n",
        "When comparing  transformer models to traditional n-gram models, you likely noticed some key differences.\n",
        "\n",
        "N-gram models predict the next token based on a fixed window of the preceding tokens (e.g., the last two or three words). These models often struggle with longer-range dependencies or more complex sentence structures, as they only consider a limited context.\n",
        "\n",
        "In contrast, transformer models have a very long context window usually of thousands of tokens. The context window (or length) represents the size of prior texts that the transformer model can consider and focus on at a given time. As a result, the transformer model is better able to learn the relationship between several tokens when compared to N-gram models, which can realistically only go up to single-digit context lengths. This makes the transformer models more flexible and accurate, especially in situations where the context stretches beyond just a few words.\n",
        "\n",
        "For example, when comparing outputs for the same prompt, you may see that n-gram models often fail to predict more specific words (like `'water'` or `'food'` after `'hungry'`), because they don't understand the broader context as effectively. Transformer models, on the other hand, would likely generate more contextually appropriate words, like `'food'` when the prompt mentions hunger or `'water'` when thirst is implied."
      ],
      "metadata": {
        "id": "R-ce_mgGAq76"
      }
    },
    {
      "metadata": {
        "id": "U6dyJeR3Zz3e"
      },
      "cell_type": "markdown",
      "source": [
        "### Generating more samples\n",
        "\n",
        "Now, try increasing the `num_next_tokens` to generate more texts and observe how the model responds:"
      ]
    },
    {
      "metadata": {
        "id": "wPVIFr68Z1vT"
      },
      "cell_type": "code",
      "source": [
        "model_name = 'Gemma-1B' #@param [ 'Gemma-1B', 'Gemma-4B']\n",
        "\n",
        "prompt = 'Jide was thirsty so she went looking for' #@param {type: 'string'}\n",
        "\n",
        "num_next_tokens = 100 #@param {type: 'number'}\n",
        "\n",
        "output_text, next_token_logits, tokenizer = prompt_transformer_model(prompt,\n",
        "                                                                     max_new_tokens=num_next_tokens,\n",
        "                                                                     model_name=model_name)\n",
        "clear_output() # Clears the output.\n",
        "\n",
        "print(output_text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Fggmt1p4Xn1y"
      },
      "cell_type": "markdown",
      "source": [
        "#### Language models tend to follow their training data distributions\n",
        "\n",
        "When you ran the cell above multiple times, what did you notice?\n",
        "\n",
        "- Did you observe stereotypical outputs?\n",
        "- Perhaps the output seems non-contextually relevant?\n",
        "\n",
        "Language models are adept at predicting the next token, but they closely follow the distribution of their training data. If the model is trained on biased data, it will produce biased outputs.\n",
        "\n",
        "Similarly, if a language model is trained on data gathered from across the internet, it will reflect the dominant texts and perspectives found there. For instance, if more text data is written in English than another language, then naturally the output token distributions following a prompt like `'Jide'` would be in English. Adapting or fine-tuning pre-trained language models for specific tasks (like auto-completing `'Jide...'` in a specific writing style) is an exciting research area.\n",
        "\n",
        "> **NOTE:** The `Gemma` models that are used here are pre-trained checkpoints and not \"instruction-tuned\" models. If you are curious, Gemma's <a href='https://github.com/google-deepmind/gemma'>code</a> and\n",
        "<a href='https://gemma-llm.readthedocs.io/en/latest/index.html'>documentation</a> are publicly available.\n"
      ]
    },
    {
      "metadata": {
        "id": "aGF0ds-NiBYt"
      },
      "cell_type": "markdown",
      "source": [
        "**Does the output above change every time you run the cell?**\n",
        "\n",
        "You likely noticed that the output of the transformer model changes each time you run the cell above, even with the same prompt. This is because the model uses a probability distribution to pick the next token, which introduces a level of stochasticity (randomness) into the prediction. This is the same as what you saw in the n-gram models, where the next word isn't always the same due to the model sampling from a set of possibilities.\n",
        "\n",
        "This variability helps the model generate more diverse and creative outputs like you've seen previously when sampling from the n-gram model.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "FUs4waCjOO_f"
      },
      "cell_type": "markdown",
      "source": [
        "### Controlling the model's output\n",
        "\n",
        "The `do_sample` variable is used to instruct the model to sample from the probability distribution. It is set to `True` by default. Therefore, you may observe diverse outputs when you experiment with the models above.\n",
        "\n",
        "If you want the model to return a determinstic output, i.e., to always pick the token with the highest probability of occuring next, set the variable `do_sample=False` as done in the cell below.\n",
        "\n",
        "<!--The following is an example of the model with the variable set to False.\n",
        "\n",
        "```python\n",
        "prompt_transformer_model(prompt, max_new_tokens=num_next_tokens, model_name=model_name, do_sample=False)\n",
        "```\n",
        "\n",
        "With this setting, the output will be consistent across multiple runs for the same prompt as it always selects the most probable token.\n",
        "\n",
        "**Sampling Mode (Default: `do_sample=True`)**\n",
        "-->\n",
        "\n",
        "<!--By default, `do_sample` is set to `True` making the model samples from the probability distribution which inturn introduces randomness and results in more varied and creative outputs. This is helpful when you want the model to explore a range of possible continuations for a prompt, rather than sticking strictly to the most likely outcome.-->\n",
        "\n",
        "Running the cell below multiple times with the same prompt will return the same output.\n",
        "\n",
        "Run the cell below multiple times and observe the result:"
      ]
    },
    {
      "metadata": {
        "id": "Rv7PzJFQdXiP"
      },
      "cell_type": "code",
      "source": [
        "model_name = 'Gemma-1B' #@param ['Gemma-1B', 'Gemma-4B']\n",
        "\n",
        "prompt = 'Jide was thirsty so she went looking for' #@param {type: 'string'}\n",
        "\n",
        "num_next_tokens = 100 #@param {type: 'number'}\n",
        "\n",
        "output_text, next_token_logits, tokenizer = prompt_transformer_model(prompt,\n",
        "                                                                     max_new_tokens=num_next_tokens,\n",
        "                                                                     model_name=model_name,\n",
        "                                                                     do_sample=False)\n",
        "clear_output() # Clears the output.\n",
        "\n",
        "print(output_text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "MV6Qd59qFuq-"
      },
      "cell_type": "markdown",
      "source": [
        "**Balancing creativity and consistency**\n",
        "\n",
        "Sampling from a probability distribution allows the transformer model to explore a range of possible next tokens, fostering creativity and generating varied outputs. This approach contrasts with always picking the token with the highest probability, which focuses on the most likely next token, as you have seen above.\n",
        "\n",
        "Different applications require different settings for this balance. For creative tasks such as generating stories, sampling from the probability distribution is ideal. This is because it allows the model to explore various possibilities and produce more imaginative results.\n",
        "\n",
        "If accuracy, consistency and reliability are important for your use case, it's better to choose the token with the highest probability."
      ]
    },
    {
      "metadata": {
        "id": "LU-cx5lKF4OS"
      },
      "cell_type": "markdown",
      "source": [
        "## Reflection\n",
        "\n",
        "This is the end of **Lab 3: Experiment with a Transformer Model.**\n",
        "\n",
        "In this lab, you:\n",
        "\n",
        "- Experimented with a transformer model, exploring its ability to predict the next token in a sequence. By trying different prompts and model sizes, you likely observed how the model's predictions and their probabilities shifted based on the context.\n",
        "\n",
        "- Visualized the probability distribution to have a deeper understanding of the model's confidence in different potential next tokens based on given context (prior words).\n",
        "\n",
        "- Explored the impact of generating longer sequences of text, discovering the role of sampling in creating varied and sometimes unexpected outputs. This variability showcases the creative potential of transformer models while also highlighting their susceptibility to biases present in their training data. By toggling the `do_sample` parameter, you experienced the trade-off between generating consistent, predictable outputs and embracing the diversity of sampled predictions.\n",
        "\n",
        "This lab offered a hands-on introduction to the power and nuances of transformer models. In the next part of the course, you'll compare n-grams and transformers, evaluating their performance based on fluency, coherence, and relevance. This will further solidify your understanding of these language models and their application in natural language processing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
