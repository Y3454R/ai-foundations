{
  "cells": [
    {
      "metadata": {
        "id": "McySAWvt09wn"
      },
      "cell_type": "markdown",
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "metadata": {
        "id": "6bW1AyB_f7eo"
      },
      "cell_type": "markdown",
      "source": [
        "# **Build Your Own Small Language Model, Lab 5: Training Your Own Small Language Model**\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/introduction_to_language_modeling_lab_5.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sYqG7iVwXEX0"
      },
      "cell_type": "code",
      "source": [
        "# Packages used.\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "N1XAXcZ9S-4w"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load the dataset\n",
        "\n",
        "To begin, you will load the dataset. This lab uses the [Africa Galore](https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json) dataset as in the previous lab."
      ]
    },
    {
      "metadata": {
        "id": "5mB31TuPSoWb"
      },
      "cell_type": "code",
      "source": [
        "africa_galore = pd.read_json('https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json')\n",
        "train_dataset = africa_galore['description'].values\n",
        "print('Training dataset contains', train_dataset.shape[0], 'paragraphs.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "qsgXxQ8KGNfT"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Tokenize the text\n",
        "\n",
        "At the end of the previous lab, you created a `SimpleWordTokenize` class. Below, you will use it to tokenize the text dataset:"
      ]
    },
    {
      "metadata": {
        "id": "LMK-adL0su6i"
      },
      "cell_type": "code",
      "source": [
        "# Putting it all together.\n",
        "class SimpleWordTokenizer:\n",
        "    \"\"\"A simple word tokenizer that can be initialized with texts\n",
        "       or using a provided vocabulary list.\n",
        "\n",
        "    The tokenizer splits the text sequence based on whitespace,\n",
        "    using the `encode` method to convert the text into a sequence of indices\n",
        "    and the `decode` method to convert indices back into text.\n",
        "\n",
        "    Typical usage example:\n",
        "\n",
        "        text = 'Hello there!'\n",
        "        tokenizer = SimpleWordTokenizer(text)\n",
        "        print(tokenizer.encode('Hello'))\n",
        "\n",
        "    Attributes:\n",
        "        texts: Input text dataset.\n",
        "        vocab: A pre-defined vocabulary. Defaults to None. If None,\n",
        "               the vocab is automatically inferred from the texts.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define constants.\n",
        "    UNKNOWN_TOKEN = '<UNK>'\n",
        "    PAD_TOKEN = '<PAD>'\n",
        "\n",
        "    def __init__(self, texts: list[str], vocab: list[str] | None = None):\n",
        "        \"\"\"Initializes the tokenizer with texts or using a provided vocabulary.\n",
        "\n",
        "        Args:\n",
        "          texts: Input text dataset.\n",
        "          vocab: A pre-defined vocabulary. Defaults to None. If None,\n",
        "                the vocab is automatically inferred from the texts.\n",
        "        \"\"\"\n",
        "\n",
        "        if vocab is None:\n",
        "            # Build the vocab from scratch.\n",
        "            if isinstance(texts, str):\n",
        "              texts = [texts]\n",
        "\n",
        "            # Convert text sequence to tokens.\n",
        "            tokens = [token for text in texts\n",
        "                      for token in self.split_text(text)]\n",
        "\n",
        "            # Create a vocabulary comprising of unique tokens.\n",
        "            vocab = self.build_vocab(tokens)\n",
        "\n",
        "            # Add special unknown and pad token to the vocabulary list.\n",
        "            self.vocab = [self.PAD_TOKEN] + vocab +  [self.UNKNOWN_TOKEN]\n",
        "\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        # Size of vocabulary.\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        # Create token-to-index and index-to-token mappings.\n",
        "        self.token_to_index = {token: index\n",
        "                               for index, token in enumerate(self.vocab)}\n",
        "        self.index_to_token = {index: token\n",
        "                               for index, token in enumerate(self.vocab)}\n",
        "\n",
        "        # Map the special tokens to their IDs.\n",
        "        self.pad_token_id = self.token_to_index[self.PAD_TOKEN]\n",
        "        self.unknown_token_id = self.token_to_index[self.UNKNOWN_TOKEN]\n",
        "\n",
        "    def split_text(self, text: str) -> list[str]:\n",
        "        \"\"\"Splits a given text on whitespace into tokens.\"\"\"\n",
        "        return text.split(' ')\n",
        "\n",
        "    def join_text(self, text_lists: list[str]) -> str:\n",
        "        \"\"\"Combines a list of tokens into a single string,\n",
        "            with tokens separated by spaces.\n",
        "        \"\"\"\n",
        "        return ' '.join(text_lists)\n",
        "\n",
        "    def build_vocab(self, tokens: list[str])-> list[str]:\n",
        "      \"\"\"Create a vocabulary list from the set of tokens.\"\"\"\n",
        "      return list(set(tokens))\n",
        "\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        \"\"\"Encodes a text sequence into a list of indices based on the\n",
        "           vocabulary.\n",
        "\n",
        "        Args:\n",
        "            text: The input text to be encoded.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of indices corresponding to the tokens in the\n",
        "                  input text.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert tokens into indexes.\n",
        "        return [self.token_to_index.get(token,\n",
        "                                        self.token_to_index[self.UNKNOWN_TOKEN])\n",
        "                for token in self.split_text(text)]\n",
        "\n",
        "    def decode(self, numbers: int | list[int]) -> str:\n",
        "        \"\"\"Decodes a list (or single index) of integers back into\n",
        "        corresponding tokens from the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            numbers: A single index or a list of indices to be\n",
        "                     decoded into tokens.\n",
        "\n",
        "        Returns:\n",
        "            str: A string of decoded tokens corresponding to the input indices.\n",
        "        \"\"\"\n",
        "\n",
        "        # If a single integer is passed, convert it into a list.\n",
        "        if isinstance(numbers, int):\n",
        "            numbers = [numbers]\n",
        "\n",
        "        # Map indices to tokens.\n",
        "        tokens = [self.index_to_token.get(number, self.unknown_token_id)\n",
        "                  for number in numbers]\n",
        "\n",
        "        # Join the decoded tokens into a single string.\n",
        "        return self.join_text(tokens)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "WO8JZa1CTHfO"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleWordTokenizer(train_dataset)\n",
        "encoded_tokens = [tokenizer.encode(text) for text in train_dataset]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "TsqOz4SQIrOf"
      },
      "cell_type": "markdown",
      "source": [
        "Now, make sure to check the length of the encoded tokens:"
      ]
    },
    {
      "metadata": {
        "id": "0gNagWwiIo0o"
      },
      "cell_type": "code",
      "source": [
        "print(len(encoded_tokens))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "shQvu8OqIuGY"
      },
      "cell_type": "markdown",
      "source": [
        "Next, examine the first ten token IDs of the first tokenized paragraph in the train dataset:"
      ]
    },
    {
      "metadata": {
        "id": "XWRFQkRFI1df"
      },
      "cell_type": "code",
      "source": [
        "encoded_tokens[0][:10]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "38uEXAhL6Rmc"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Pad or truncate the tokens to the desired length\n"
      ]
    },
    {
      "metadata": {
        "id": "H2HzW94G7PVG"
      },
      "cell_type": "markdown",
      "source": [
        "The padding `'<PAD>'` token is used to ensure that all sequences have the same length. The paragraphs have varying lengths but neural networks expect inputs to have a uniform shape. Shorter paragraphs need to be padded to match the longest paragraph, so that all inputs to the network follow the same dimensions. The transformer model takes in each paragraph as its context and learns the relationship between the tokens."
      ]
    },
    {
      "metadata": {
        "id": "sWA-cd-K_ql3"
      },
      "cell_type": "markdown",
      "source": [
        "This part checks the length of the first paragraph:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "e_RgRB6gvL6B"
      },
      "cell_type": "code",
      "source": [
        "print('length of first paragraph:', len(encoded_tokens[0] ))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "oCEiSccJ9Y-3"
      },
      "cell_type": "markdown",
      "source": [
        "Count the maximum and minimum number of tokens in a paragraph in the dataset to determine the length to pad up to:"
      ]
    },
    {
      "metadata": {
        "id": "LY2hke_k_LT0"
      },
      "cell_type": "code",
      "source": [
        "shortest_paragragh_length = len(min(encoded_tokens, key=len))\n",
        "longest_paragragh_length = len(max(encoded_tokens, key=len))\n",
        "print(f'length of the shortest paragraph is:', shortest_paragragh_length)\n",
        "print(f'length of the longest paragraph is:', longest_paragragh_length)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "amMbFyNtTny5"
      },
      "cell_type": "markdown",
      "source": [
        "As discussed earlier, all paragraphs are required to be the same length to prepare the training dataset. One option is to truncate longer paragraphs to match the shortest one. While this approach is efficient, it risks losing important context since longer paragraphs would have tokens cut off.\n",
        "\n",
        "Another option is to pad the shorter paragraphs with the special `'<PAD>'` token, making all paragraphs the same length as the longest one. This method ensures that each paragraph retains the full context. However, while padding helps maintain meaning, it may introduce extra memory and computation overhead.\n",
        "\n",
        "Alternatively, the distribution of paragraph lengths can be analyzed to choose a padding length that covers most of the content, which avoids excessive padding. This approach helps balance context retention with performance optimization. The next section provides flexibility in adjusting how much padding to add, but if you set it too low you will effectively be truncating the paragraphs.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0c5Vsplt_L8b"
      },
      "cell_type": "markdown",
      "source": [
        "Use the box below to enter the maximum length you want to pad your paragraphs to. To understand how this works, try entering the minimum paragraph length you computed earlier, then run the cell and observe the output. After that, try entering the maximum paragraph length. If your dataset has short paragraphs, you can simply pad them up to the maximum paragraph length. However, if you start training your model and encounter an \"out of memory\" error, you can return to this part and reduce the maximum length if the maximum length you used is too large:"
      ]
    },
    {
      "metadata": {
        "id": "m-AjAUdVvDjm"
      },
      "cell_type": "code",
      "source": [
        "maxlen = 320 #@param {type: 'number'}\n",
        "\n",
        "# Ensure that maxlen is positive.\n",
        "assert maxlen > 0, 'Max length must be greater than 0. Increase the `maxlen`'\n",
        "assert maxlen <= longest_paragragh_length, ('Note: The padding token '\n",
        "       f'{tokenizer.pad_token_id} will be added to sequences longer than the'\n",
        "       'longest paragraph - You probably don\"t want that. Reduce the `maxlen`')\n",
        "\n",
        "# Check if maxlen is shorter or longer than the longest paragraph.\n",
        "if maxlen < longest_paragragh_length:\n",
        "    print('\\033[33mWarning: The longest paragraph has '\n",
        "    f'{longest_paragragh_length} tokens, but `maxlen` '\n",
        "    f'is set to {maxlen}. As a result, paragraphs longer than '\n",
        "    '`maxlen` will be truncated.\\033[0m')\n",
        "\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    encoded_tokens,\n",
        "    maxlen=maxlen,\n",
        "    padding='post',\n",
        "    truncating='post',\n",
        "    value=tokenizer.pad_token_id)\n",
        "\n",
        "print('New length of first paragraph:', len(padded_sequences[0]), '\\n')\n",
        "\n",
        "print('Padding makes the length of all sequences the same as the specified ' +\n",
        "      '`maxlen`')\n",
        "\n",
        "print('Notice the first 10 tokens observed above appear after the '\n",
        "      f'padded token {tokenizer.pad_token_id} \\n')\n",
        "print('Padded tokens of first paragraph:\\n', padded_sequences[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cgIcROI8FurO"
      },
      "cell_type": "code",
      "source": [
        "print('A different paragraph looks like this after padding:\\n', padded_sequences[-1])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "NAZzTjHmwaO_"
      },
      "cell_type": "code",
      "source": [
        "padded_sequences.shape"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "81N0QsKuEPhq"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4: Prepare input and target\n",
        "\n",
        "It is important to review the inputs and targets for the transformer model and  how to prepare them.\n",
        "\n",
        "The model works *autoregressively*. This means it generates one token at a time and uses the previously generated tokens as context to predict the next one. Therefore, you need to organize the data in a specific way to train the model:\n",
        "\n",
        "- **Input**: The input is a sequence of tokens that is fed into the transformer model. This can be part of a paragraph, a full paragraph, or even multiple paragraphs, depending on how the data is structured.\n",
        "  \n",
        "- **Target**: The target sequence is what you want the model to predict. The target will be the same as the input sequence, but *shifted left by one token*. This means the target will contain the next token that should follow the input sequence.\n",
        "\n",
        "For example:\n",
        "- Input: \"The cat sat\"\n",
        "- Target: \"cat sat on\" (shifted by one token)\n",
        "\n",
        "This setup helps the model learn how to predict the next token in the sequence based on the context provided by the previous ones.\n",
        "\n",
        "This method of using input and target or label sequences is a common approach in *supervised learning*, where the model is trained to predict the target given the input:\n"
      ]
    },
    {
      "metadata": {
        "id": "JLmeuOr2obyr"
      },
      "cell_type": "code",
      "source": [
        "# Prepare input and target to the transformer model.\n",
        "input = padded_sequences[:, :-1] # All tokens except the last one.\n",
        "target = padded_sequences[:, 1:]  # All tokens except the first one."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Xc2IrUF-ovZN"
      },
      "cell_type": "markdown",
      "source": [
        "Print out the first ten token IDs of the first input and target sequence:"
      ]
    },
    {
      "metadata": {
        "id": "QCAQlnbrowAz"
      },
      "cell_type": "code",
      "source": [
        "print(input[0, :10])\n",
        "print(target[0, :10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "DUfcnq9qtErc"
      },
      "cell_type": "markdown",
      "source": [
        "Pay attention to how the input and target are shifted by 1."
      ]
    },
    {
      "metadata": {
        "id": "OkUHG2TOs5Ki"
      },
      "cell_type": "markdown",
      "source": [
        "Now, decode the numbers to visualize the texts that are shifted:\n"
      ]
    },
    {
      "metadata": {
        "id": "01e4DtRNsyhz"
      },
      "cell_type": "code",
      "source": [
        "# Decodes the first 10 tokens of the first paragraph from input.\n",
        "tokenizer.decode(input[0, :10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "mFN2ENu1s1SB"
      },
      "cell_type": "code",
      "source": [
        "# Decodes the first 10 tokens of the first paragraph from target.\n",
        "tokenizer.decode(target[0, :10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "NGWjVLm6WCHi"
      },
      "cell_type": "markdown",
      "source": [
        "Print the input and output shape:\n",
        "- This returns a tuple of sequence length (the number of paragraphs you selected) and maximum length (the maximum length enforced through padding).\n",
        "- The shape is reduced by 1, as the input and target are shifted by 1."
      ]
    },
    {
      "metadata": {
        "id": "Xl5qOyNUpdhF"
      },
      "cell_type": "code",
      "source": [
        "input.shape, target.shape"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "107RTtKWCpV2"
      },
      "cell_type": "markdown",
      "source": [
        "Update the maxlen to reflect that the input and target take sequences that are one token shorter:"
      ]
    },
    {
      "metadata": {
        "id": "NWSao54yCk9M"
      },
      "cell_type": "code",
      "source": [
        "maxlen = input.shape[1]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HUty-eHlCTXy"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 5: Shuffle the dataset and specify the batch size"
      ]
    },
    {
      "metadata": {
        "id": "NutUoSXCSMQI"
      },
      "cell_type": "markdown",
      "source": [
        "Neural network training involves selecting a number of random examples, called a `batch`, from the dataset. The dataset is shuffled to achieve this randomness and the size of the batch is determined by the `batch_size`.\n",
        "\n",
        "\n",
        "The figure below illustrates a dataset with seven examples, where each example is padded to `maxlen`, the length of the longest example. The dataset is then shuffled, and a batch of size three is created, with the final batch containing only one example.\n",
        "\n",
        "\n",
        "<!--Below is an illustration of how data is prepared for training a neural network when dealing with sequences of varying lengths. First, each example (in this case, a paragraph) may have a different size, so we apply padding to make them uniformly long. After padding, we shuffle the data to avoid feeding examples in a fixed order. Finally, we group these shuffled and padded examples into batches, each containing `batch_size` examples: -->"
      ]
    },
    {
      "metadata": {
        "id": "lS-tO1YBc4Qk"
      },
      "cell_type": "markdown",
      "source": [
        "<img src='https://storage.googleapis.com/dm-educational/assets/ai_foundations/evolve_graphic.png' width='1000'>"
      ]
    },
    {
      "metadata": {
        "id": "9yazOaI7XVlx"
      },
      "cell_type": "markdown",
      "source": [
        "*Run the cell below to shuffle and create batches of examples:*"
      ]
    },
    {
      "metadata": {
        "id": "CYdVzdHvM6w-"
      },
      "cell_type": "code",
      "source": [
        "# (1) Create TensorFlow dataset to prepare sequence.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input, target))\n",
        "\n",
        "# (2) Randomly shuffle the dataset.\n",
        "#     The buffer_size determines how many examples from the dataset\n",
        "#     are held in memory before shuffling.\n",
        "#     If you're working with a very large dataset,\n",
        "#     reduce the buffer_size as needed.\n",
        "dataset = dataset.shuffle(buffer_size=len(input))\n",
        "\n",
        "# (3) Specify batch size.\n",
        "batch_size = 32  #@param {type: 'number'}\n",
        "\n",
        "# (4) Create batches.\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "for batch in dataset.take(1):\n",
        "    print(batch)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "rVELGmoNmR3g"
      },
      "cell_type": "markdown",
      "source": [
        "Count the total number of batches:"
      ]
    },
    {
      "metadata": {
        "id": "0sc2lVvlktGH"
      },
      "cell_type": "code",
      "source": [
        "total_batches = 0\n",
        "for batch in dataset:\n",
        "    total_batches += 1\n",
        "print('Total number of batches is:', total_batches)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6nwdEqyCfA_t"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 6: Train a small language model (SLM)"
      ]
    },
    {
      "metadata": {
        "id": "f4dsdg-DtW64"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this next step, you will train a language model with around four million parameters, which is far smaller in size compared to production systems like Google Gemini (with billions of parameters). These are referred to as large language models (LLMs).\n",
        "\n",
        "It's important to note that the size of the transformer model has an impact on its performance. Larger models with more parameters, have the capacity to learn more complex patterns and deliver better accuracy. However, they also require more computational resources, memory, and processing power, which can lead to longer training times (how long the model needs to update to reach optimal performance) and higher costs.\n",
        "\n",
        "\n",
        "**What are parameters?**\n",
        "\n",
        "Parameters refer to a set of numbers in a machine learning model that are adjusted during training in order to perform the training task. Our language model updates its parameters after processing each batch of training data to better predict the next token given a context (prior tokens). At the start of the training, the parameters are random numbers, and during each training iteration, the model updates these numbers such that it gets better at predicting the next token."
      ]
    },
    {
      "metadata": {
        "id": "4bUco9P0WFOL",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "# @title Hidden code used for training and sampling from the trained model.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from typing import Any\n",
        "import plotly.express as px\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import ops, layers\n",
        "\n",
        "os.environ['KERAS_BACKEND'] = 'jax'\n",
        "tf.random.set_seed(812)  # For TensorFlow operations.\n",
        "keras.utils.set_random_seed(812)  # For Keras layers.\n",
        "\n",
        "\n",
        "def create_model(vocab_size: int,\n",
        "                 maxlen: int,\n",
        "                 d_model: int = 256,\n",
        "                 ff_dim: int = 256,\n",
        "                 num_heads: int = 2,\n",
        "                 n_blocks: int = 1,\n",
        "                 optimizer: str = 'adamw',\n",
        "                 learning_rate: float = 1e-4,\n",
        "                 dropout_rate: float = 0.0,\n",
        "                 activation: str = 'relu',\n",
        "                 pad_token_id: int = 0) -> keras.Model:\n",
        "    \"\"\"Creates a transformer-based model for sequence processing tasks.\n",
        "\n",
        "    Example:\n",
        "        model = create_model(vocab_size=5000, maxlen=100,\n",
        "                            embed_dim=256, ff_dim=512,\n",
        "                            num_heads=8, n_blocks=2)\n",
        "        print(model.summary())\n",
        "\n",
        "    Notes:\n",
        "        - The model uses causal (masked) attention to ensure that each token\n",
        "          only attends to previous tokens and not future tokens.\n",
        "        - The final dense layer produces a logit over the vocabulary for\n",
        "          each token in the sequence.\n",
        "        - The loss function is `CustomMaskPadLoss`, which ignores padding\n",
        "          tokens in the loss computation.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: The size of the vocabulary, i.e.,\n",
        "                    the number of unique tokens.\n",
        "        maxlen: The maximum length of the input sequences.\n",
        "        d_model: The dimensionality of the embedding space.\n",
        "                   Default is 256.\n",
        "        ff_dim: The number of units in the feed-forward network\n",
        "                of each transformer block. Default is 256.\n",
        "        num_heads: The number of attention heads in the multi-head\n",
        "                   attention mechanism. Default is 2.\n",
        "        n_blocks: The number of transformer blocks to stack in the model.\n",
        "                  Default is 1.\n",
        "        optimizer: The optimizer to use for training, either 'adamw'\n",
        "                   ('adam with weight decay) or 'sgd'.\n",
        "                   Default is 'adamw'.\n",
        "        learning_rate: The learning rate for the optimizer. Default is 1e-4.\n",
        "        dropout_rate: The dropout rate to prevent overfitting.\n",
        "                       Default is 0.1 (no dropout).\n",
        "        activation: The activation function to use in the feed-forward network\n",
        "                    of each Transformer block. Default is 'relu'.\n",
        "        pad_token_id: The ID used to represent padding tokens in the sequence.\n",
        "                      This is used to mask padded tokens in the loss\n",
        "                      calculation. Default is 0.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: The compiled Keras model which outputs the probability\n",
        "                      of the next token prediction.\n",
        "\n",
        "\n",
        "    Raises:\n",
        "        NotImplementedError: If an unsupported optimizer is specified.\n",
        "    \"\"\"\n",
        "    # Create input layer.\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype='int32')\n",
        "\n",
        "    # Embedding layer that combines token and positional embeddings.\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, d_model)\n",
        "    x = embedding_layer(inputs)\n",
        "\n",
        "    # Apply a stack of transformer blocks.\n",
        "    for _ in range(n_blocks):\n",
        "        transformer_block = TransformerBlock(d_model,\n",
        "                                            num_heads,\n",
        "                                            ff_dim,\n",
        "                                            dropout_rate=dropout_rate,\n",
        "                                            activation=activation)\n",
        "        x = transformer_block(x)\n",
        "\n",
        "    # Apply dense layer, it returns raw logit of next token prediction.\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "\n",
        "    # Apply softmax to turn raw logit to probability distribution.\n",
        "    outputs = layers.Softmax()(outputs)\n",
        "\n",
        "    # Build the model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Set up optimizer based on input string.\n",
        "    optimizer_instance = get_optimizer(optimizer, learning_rate)\n",
        "\n",
        "    # Define the loss function and compile the model.\n",
        "    loss_fn = CustomMaskPadLoss(pad_token_id=pad_token_id)\n",
        "    model.compile(optimizer=optimizer_instance, loss=loss_fn)\n",
        "\n",
        "    # Final output layer returns the probability of next token prediction.\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_optimizer(optimizer_name: str,\n",
        "                  learning_rate: float) -> keras.optimizers.Optimizer:\n",
        "    \"\"\"Helper function to get the appropriate optimizer instance.\n",
        "\n",
        "    Args:\n",
        "        optimizer_name: The optimizer type ('adam' or 'sgd').\n",
        "        learning_rate: The learning rate for the optimizer.\n",
        "\n",
        "    Returns:\n",
        "        keras.optimizers.Optimizer: The corresponding optimizer instance.\n",
        "\n",
        "    Raises:\n",
        "        NotImplementedError: If an unsupported optimizer is specified.\n",
        "    \"\"\"\n",
        "    if optimizer_name.lower() == 'sgd':\n",
        "        return keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    elif optimizer_name.lower() == 'adamw':\n",
        "        return keras.optimizers.AdamW(learning_rate=learning_rate,\n",
        "                                      weight_decay=0.005,\n",
        "                                      gradient_accumulation_steps=None\n",
        "                                      )\n",
        "    else:\n",
        "        raise NotImplementedError(f'Optimizer {optimizer_name}'\n",
        "                                  ' is not implemented.')\n",
        "\n",
        "\n",
        "# Decorator so that the custom class can be saved and loaded correctly.\n",
        "@keras.saving.register_keras_serializable()\n",
        "class CustomMaskPadLoss(keras.losses.Loss):\n",
        "    \"\"\"Custom loss function for masked padding in sequence-based tasks.\n",
        "\n",
        "    This loss function computes the SparseCategoricalCrossentropy\n",
        "    loss while ignoring the padding tokens (specified by `pad_token_id`).\n",
        "    The padding tokens are not included in the loss calculation,\n",
        "    allowing the model to focus on meaningful tokens during training.\n",
        "\n",
        "    Attributes:\n",
        "        name: The name of the loss function, used by Keras.\n",
        "              Defaults to 'custom_mask_pad_loss'.\n",
        "        pad_token_id: The ID of the padding token. If provided,\n",
        "                      padding tokens will be ignored during loss calculation.\n",
        "                      If None, no padding is masked.\n",
        "        kwargs: Additional keyword arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 name: str = 'custom_mask_pad_loss',\n",
        "                 pad_token_id: int | None = None,\n",
        "                 **kwargs: dict):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "    def call(self,\n",
        "             y_true: tf.Tensor,\n",
        "             y_pred: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Computes the custom loss, optionally masking the padding\n",
        "           tokens and normalizing the loss by the number of non-masked tokens.\n",
        "           The loss is computed using the SparseCategoricalCrossentropy\n",
        "           loss function.\n",
        "        \"\"\"\n",
        "        loss_fn =  tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                        # The model's output is a probability distribution. If\n",
        "                        # it is raw logit, this should be True.\n",
        "                        from_logits=False,\n",
        "\n",
        "                        # Average the loss across the batch size.\n",
        "                        reduction='sum_over_batch_size'\n",
        "                    )\n",
        "\n",
        "        if self.pad_token_id is not None:\n",
        "            # Create a boolean mask: True for non-padding tokens.\n",
        "            # Shape: (batch_size, sequence_length)\n",
        "            mask = tf.not_equal(y_true, self.pad_token_id)\n",
        "\n",
        "            # Use tf.boolean_mask to filter out padded tokens.\n",
        "            # y_true_filtered will be a 1D tensor containing only\n",
        "            # the valid token labels.\n",
        "            y_true_filtered = tf.boolean_mask(y_true, mask)\n",
        "\n",
        "            # y_pred_filtered will be a 2D tensor containing only\n",
        "            # the predictions for valid tokens.\n",
        "            y_pred_filtered = tf.boolean_mask(y_pred, mask)\n",
        "\n",
        "            loss = loss_fn(y_true_filtered, y_pred_filtered)\n",
        "        else:\n",
        "            loss = loss_fn(y_true, y_pred)\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Decorator so that the custom class can be saved and loaded correctly.\n",
        "@keras.saving.register_keras_serializable()\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    \"\"\"Combines token embeddings with positional embeddings.\n",
        "\n",
        "    This layer creates combined token and positional embeddings\n",
        "    for input sequences.\n",
        "    The `mask_zero=True` setting in the token embeddings allows for\n",
        "    automatic masking of padded tokens.\n",
        "\n",
        "    Attributes:\n",
        "        maxlen: The maximum expected sequence length. This determines the\n",
        "                    range of positional embeddings.\n",
        "        vocab_size: The size of the vocabulary. This determines the size\n",
        "                        of the token embedding matrix.\n",
        "        d_model: The dimensionality of the token and positional embeddings.\n",
        "        positional_embedding_type: The type of positional embedding\n",
        "                                                to use.  Can be 'simple',\n",
        "                                                'sinusoidal'.\n",
        "                                                Defaults to 'sinusoidal'.\n",
        "        kwargs: Additional keyword arguments passed to the base\n",
        "                `keras.layers.Layer` constructor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, maxlen: int,\n",
        "                vocab_size: int,\n",
        "                d_model: int,\n",
        "                positional_embedding_type: str = 'sinusoidal',\n",
        "                **kwargs: dict):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.maxlen = maxlen\n",
        "        self.positional_embedding_type=positional_embedding_type\n",
        "\n",
        "        # Set mask_zero=True so that Keras generates a mask for padded tokens.\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size,\n",
        "                                          output_dim=d_model,\n",
        "                                          mask_zero=True)\n",
        "\n",
        "        if self.positional_embedding_type == 'simple':\n",
        "          self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=d_model)\n",
        "        elif self.positional_embedding_type == 'sinusoidal':\n",
        "          self.pos_emb = self.positional_encoding(length=maxlen, depth=d_model)\n",
        "        else:\n",
        "            raise NotImplementedError('Positional embedding type'\n",
        "                                      f' {self.positional_embedding_type}'\n",
        "                                      f' not implemented.')\n",
        "\n",
        "    def positional_encoding(self,length: int, depth: int) -> tf.Tensor:\n",
        "        \"\"\"Creates a positional encoding for a sequence of tokens.\n",
        "            This approach uses sine and cosine functions at varying\n",
        "            frequencies to create\n",
        "            a unique positional representation for each token in the sequence.\n",
        "\n",
        "        Args:\n",
        "          length: The length of the sequence (number of tokens).\n",
        "          depth: The dimensionality of the encoding (must be even).\n",
        "\n",
        "        Returns:\n",
        "          A TensorFlow tensor of shape (length, depth) representing\n",
        "          the positional encoding.\n",
        "        \"\"\"\n",
        "        depth = depth // 2  # Use integer division to ensure an integer depth.\n",
        "\n",
        "        positions = np.arange(length)[:, np.newaxis]  # (seq, 1)\n",
        "        depths = np.arange(depth)[np.newaxis, :] / depth  # (1, depth)\n",
        "\n",
        "        angle_rates = 1 / (10000**depths)  # (1, depth)\n",
        "        angle_rads = positions * angle_rates  # (pos, depth)\n",
        "\n",
        "        pos_encoding = np.concatenate(\n",
        "            [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "            axis=-1)\n",
        "\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        token_embeddings = self.token_emb(x)\n",
        "\n",
        "        if self.positional_embedding_type == 'sinusoidal':\n",
        "          # This factor sets the relative scale of the embedding\n",
        "          # and positonal_encoding.\n",
        "          token_embeddings *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "          position_embeddings= self.pos_emb[tf.newaxis, :, :]\n",
        "        else:\n",
        "          # Defaults to simple `positional_embedding_type`.\n",
        "          positions = ops.arange(0, self.maxlen, 1)\n",
        "          position_embeddings = self.pos_emb(positions)\n",
        "\n",
        "        return token_embeddings + position_embeddings\n",
        "\n",
        "\n",
        "# Decorator so that the custom class can be saved and loaded correctly.\n",
        "@keras.saving.register_keras_serializable()\n",
        "class TransformerBlock(layers.Layer):\n",
        "  \"\"\"A single Transformer block.\n",
        "\n",
        "    The Transformer block is a fundamental component of the Transformer\n",
        "    architecture, which is commonly used for sequence-based tasks. It consists\n",
        "    of a MultiHeadAttention layer followed by a feed-forward network,\n",
        "    with layer normalization and dropout applied at each step.\n",
        "\n",
        "    Example:\n",
        "        transformer_block = TransformerBlock(d_model=256, num_heads=8,\n",
        "                                             ff_dim=1024)\n",
        "        output = transformer_block(inputs)\n",
        "\n",
        "    Attributes:\n",
        "        d_model: The dimensionality of the input embedding (also the output\n",
        "                 size of the attention layer).\n",
        "        num_heads: The number of attention heads in the multi-head\n",
        "                   attention mechanism.\n",
        "        ff_dim: The number of units in the feed-forward network.\n",
        "        dropout_rate: Dropout rate, between 0 and 1. Default is 0.0\n",
        "        activation: The activation function to use in the feed-forward network.\n",
        "                     Default is 'relu'.\n",
        "        seed: Random seed for dropout and attention layers to ensure\n",
        "              reproducibility. Default is 42.\n",
        "        kwargs: Additional keyword arguments to pass to the parent `Layer`\n",
        "                class.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: The output of the Transformer block after applying the\n",
        "                   multi-head attention, feed-forward network,\n",
        "                   layer normalization, and residual connections.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               d_model: int,\n",
        "               num_heads: int,\n",
        "               ff_dim: int,\n",
        "               dropout_rate: float = 0.0,\n",
        "               activation: str = 'relu',\n",
        "               **kwargs: dict):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.self_attention = MultiHeadSelfAttention(d_model,\n",
        "                                                 num_heads,\n",
        "                                                 dropout_rate)\n",
        "    self.feed_forward = FeedForwardNetwork(d_model,\n",
        "                                           ff_dim,\n",
        "                                           dropout_rate,\n",
        "                                           activation)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Applies a single transformer block to the input tensor.\n",
        "\n",
        "    Notes:\n",
        "        - The transformer block follows the architecture with residual\n",
        "          connections and layer normalization.\n",
        "\n",
        "    Args:\n",
        "        inputs: The input tensor of shape (batch_size, seq_len, embed_dim).\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: The output tensor of shape (batch_size, seq_len, embed_dim)\n",
        "                    after applying the transformer block.\n",
        "    \"\"\"\n",
        "    # First block: masked self-attention.\n",
        "    attn_output = self.self_attention(inputs)\n",
        "\n",
        "    # Second block: feedforward network applied on attention output.\n",
        "    ffn_output = self.feed_forward(attn_output)\n",
        "\n",
        "    return ffn_output\n",
        "\n",
        "\n",
        "# Decorator so that the custom class can be saved and loaded correctly.\n",
        "@keras.saving.register_keras_serializable()\n",
        "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
        "    \"\"\"Feed forward network layer.\n",
        "\n",
        "    This layer implements a two-layer feedforward network with a residual\n",
        "    connection and layer normalization. It's a common component in\n",
        "    transformer architectures, used to introduce non-linearity and improve\n",
        "    the model's ability to capture complex relationships.\n",
        "\n",
        "    Args:\n",
        "        d_model: The dimensionality of the embedding space.\n",
        "        ff_dim: The dimensionality of the hidden layer in the feedforward\n",
        "                network (often larger than d_model).\n",
        "        dropout_rate: The dropout rate applied to the output of the feedforward\n",
        "                      network. Defaults to 0.0.\n",
        "        activation: The activation function used in the first dense layer.\n",
        "                    Defaults to 'relu'.\n",
        "        **kwargs: Additional keyword arguments passed to the base Layer.\n",
        "\n",
        "    Call Arguments:\n",
        "        x: Input tensor of shape (batch_size, sequence_length, d_model).\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Output tensor of shape (batch_size, sequence_length, d_model)\n",
        "                  after applying the feedforward network and residual connection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                d_model: int,\n",
        "                ff_dim: int,\n",
        "                dropout_rate: float = 0.0,\n",
        "                activation: str = 'relu',\n",
        "                **kwargs: dict):\n",
        "        super(FeedForwardNetwork, self).__init__(**kwargs)\n",
        "        # Define a two-layer feedforward network.\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            # Expand dimension.\n",
        "            tf.keras.layers.Dense(ff_dim, activation=activation),\n",
        "            # Project back to d_model.\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Applies the feedforward network to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, sequence_length, d_model).\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Output tensor of shape (batch_size, sequence_length,\n",
        "                                              d_model).\n",
        "        \"\"\"\n",
        "        ffn_output = self.ffn(x)\n",
        "        ffn_output = self.dropout(ffn_output)\n",
        "        # Add residual connection followed by layer normalization.\n",
        "        output = self.layernorm(x + ffn_output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Decorator so that the custom class can be saved and loaded correctly.\n",
        "@keras.saving.register_keras_serializable()\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"Multi-head self-attention Layer.\n",
        "\n",
        "    This layer implements multi-head self-attention, a key component in\n",
        "    Transformer architectures.\n",
        "    It computes attention weights for each head and applies them to the\n",
        "    input to generate a contextually enriched representation.\n",
        "\n",
        "    Args:\n",
        "        d_model: The dimensionality of the embedding space.\n",
        "        num_heads: The number of attention heads.\n",
        "        dropout_rate: The dropout rate applied to the attention output.\n",
        "                      Defaults to 0.0.\n",
        "        **kwargs: Additional keyword arguments passed to the base Layer.\n",
        "\n",
        "    Call Arguments:\n",
        "        x: Input tensor of shape (batch_size, sequence_length, d_model).\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Output tensor of shape (batch_size, sequence_length, d_model)\n",
        "                    with self-attention applied.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "               d_model: int,\n",
        "               num_heads: int,\n",
        "               dropout_rate: float = 0.0,\n",
        "               **kwargs: dict):\n",
        "        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n",
        "\n",
        "        # Multi-head self-attention layer.\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                                      key_dim=d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "      \"\"\"Applies multi-head self-attention to the input tensor.\n",
        "\n",
        "      Args:\n",
        "          x: Input tensor of shape (batch_size, sequence_length, d_model).\n",
        "\n",
        "      Returns:\n",
        "          tf.Tensor: Output tensor of shape (batch_size, sequence_length,\n",
        "                                            d_model).\n",
        "      \"\"\"\n",
        "\n",
        "      # Apply self-attention. The mask is typically a look-ahead mask.\n",
        "      attn_output = self.mha(query=x, value=x, key=x,  use_causal_mask=True)\n",
        "      attn_output = self.dropout(attn_output)\n",
        "      # Add residual connection followed by layer normalization.\n",
        "      output = self.layernorm(x + attn_output)\n",
        "      return output\n",
        "\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "\n",
        "    1. Feed a starting prompt to the model.\n",
        "    2. Predict probabilities for the next token.\n",
        "    3. Sample the next token and add it to the input for the next prediction.\n",
        "\n",
        "    Attributes:\n",
        "        max_tokens: Number of tokens to be generated after the prompt.\n",
        "        start_tokens: Token indices for the starting prompt.\n",
        "        tokenizer: Tokenizer instance to convert token indices back to words.\n",
        "        pad_token_id: Token ID for padding, default is 0.\n",
        "        print_every: Print the generated text every this many epochs.\n",
        "                     Default is 1.\n",
        "        kwargs: Any additional keyword arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_tokens: int,\n",
        "                 start_tokens: list[int],\n",
        "                 tokenizer: Any,\n",
        "                 pad_token_id: int = 0,\n",
        "                 print_every: int = 1,\n",
        "                 **kwargs: dict):\n",
        "        \"\"\"Initializes the text generator callback.\n",
        "\n",
        "        Args:\n",
        "            max_tokens: Number of tokens to generate.\n",
        "            start_tokens: Token indices for the initial prompt.\n",
        "            tokenizer: The tokenizer used to decode generated token indices.\n",
        "            pad_token_id: The padding token ID (default is 0).\n",
        "            print_every: Print the generated text every `print_every` epochs.\n",
        "                         Default is 1.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.tokenizer = tokenizer\n",
        "        self.print_every = print_every\n",
        "        self.pad_token_id = pad_token_id  # ID for padding token.\n",
        "\n",
        "    def greedy_decoding(self, probs: np.ndarray) -> int:\n",
        "        \"\"\"Select the token index with the highest probability.\n",
        "\n",
        "        Args:\n",
        "            probs: The probability distribution of next token prediction.\n",
        "\n",
        "        Returns:\n",
        "            int: The index of the predicted token with the highest probability.\n",
        "        \"\"\"\n",
        "        predicted_index = np.argmax(probs)\n",
        "        return predicted_index\n",
        "\n",
        "    def sampling(self, probs: np.ndarray) -> int:\n",
        "      \"\"\"Sample a token index from the predicted next token probability.\n",
        "\n",
        "      Args:\n",
        "          probs: The probability distribution of predicted next token.\n",
        "\n",
        "      Returns:\n",
        "          int: The index of the sampled token.\n",
        "      \"\"\"\n",
        "      return np.random.choice(np.arange(len(probs)), p=probs)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs: dict | None = None) -> None:\n",
        "        \"\"\"Generate and print text after each epoch based on the starting\n",
        "            tokens.\n",
        "\n",
        "        Args:\n",
        "            epoch: The current epoch number.\n",
        "            logs: Logs from the training process.\n",
        "        \"\"\"\n",
        "        maxlen = self.model.layers[0].output.shape[1]\n",
        "        # Make a copy of the start tokens.\n",
        "        start_tokens = list(self.start_tokens)\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            return\n",
        "\n",
        "        num_tokens_generated = 0\n",
        "        tokens_generated: list[int] = []\n",
        "\n",
        "        while num_tokens_generated < self.max_tokens:\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "\n",
        "            # Handle padding to ensure the sequence is of the correct length.\n",
        "            if pad_len < 0:\n",
        "                x = start_tokens[:maxlen]\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = start_tokens + [self.pad_token_id] * pad_len\n",
        "            else:\n",
        "                x = start_tokens\n",
        "\n",
        "            x = np.array([x])\n",
        "            y = self.model.predict(x, verbose=0)\n",
        "            sample_token = self.sampling(y[0][sample_index])\n",
        "\n",
        "            tokens_generated.append(sample_token)\n",
        "            start_tokens.append(sample_token)\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "\n",
        "        # Combine the starting tokens with the generated tokens.\n",
        "        output_tokens = self.start_tokens + tokens_generated\n",
        "        output_tokens = list(map(int, output_tokens))\n",
        "\n",
        "        # Decode and print the generated text.\n",
        "        txt = self.tokenizer.decode(output_tokens)\n",
        "        print('Generated text:\\n', txt, '\\n')\n",
        "\n",
        "\n",
        "def sampling(probs: np.ndarray) -> int:\n",
        "    \"\"\"Sample a token index from the predicted next token probability.\n",
        "\n",
        "    Args:\n",
        "        probs: The probability distribution of predicted next token.\n",
        "\n",
        "    Returns:\n",
        "        int: The index of the sampled token.\n",
        "    \"\"\"\n",
        "    return np.random.choice(np.arange(len(probs)), p=probs)\n",
        "\n",
        "\n",
        "def greedy_decoding(probs: np.ndarray) -> int:\n",
        "    \"\"\"Select the token index from the predicted next token probability.\n",
        "\n",
        "    Args:\n",
        "        probs: The probability distribution of predicted next token.\n",
        "\n",
        "    Returns:\n",
        "        int: The index of the token with the highest probability.\n",
        "    \"\"\"\n",
        "    predicted_index = np.argmax(probs)\n",
        "    return predicted_index\n",
        "\n",
        "\n",
        "def generate_text(start_prompt: str,\n",
        "                  n_tokens: int,\n",
        "                  model: keras.Model,\n",
        "                  tokenizer: object,\n",
        "                  pad_token_id: int = 0,\n",
        "                  do_sample: bool = False) -> tuple[str, list[np.ndarray]]:\n",
        "    \"\"\"Generate text based on a starting prompt using a trained model.\n",
        "\n",
        "    Args:\n",
        "        start_prompt: The initial prompt to start the generation.\n",
        "        n_tokens: The number of tokens to generate after the prompt.\n",
        "        model: The trained model to use for text generation.\n",
        "        tokenizer: The tokenizer to encode and decode text.\n",
        "        pad_token_id: The token ID used for padding (default is 0).\n",
        "        do_sample: Whether to sample from the distribution or use\n",
        "                   greedy decoding (default is False).\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text after the prompt.\n",
        "    \"\"\"\n",
        "    maxlen = model.layers[0].output.shape[1]\n",
        "\n",
        "    # Tokenize the starting prompt.\n",
        "    start_tokens = tokenizer.encode(start_prompt)\n",
        "\n",
        "    # Generate tokens.\n",
        "    tokens_generated = start_tokens + []\n",
        "    probs: list[np.ndarray] = []\n",
        "    for _ in range(n_tokens):\n",
        "        pad_len = maxlen - len(start_tokens)\n",
        "        sample_index = len(start_tokens) - 1\n",
        "        if pad_len < 0:\n",
        "            # Truncate the input sequence to fit the max context length.\n",
        "            x = start_tokens[:maxlen]\n",
        "            sample_index = maxlen - 1\n",
        "        elif pad_len > 0:\n",
        "            x = start_tokens + [pad_token_id] * pad_len  # Pad the input sequence.\n",
        "        else:\n",
        "            x = start_tokens\n",
        "\n",
        "        x = np.array([x])\n",
        "        y = model.predict(x, verbose=0)  # Get predictions from the model.\n",
        "\n",
        "        probs.append(y[0][sample_index])\n",
        "\n",
        "        # Use greedy decoding or sampling based on the flag.\n",
        "        if not do_sample:\n",
        "            sample_token = greedy_decoding(y[0][sample_index])\n",
        "        else:\n",
        "            sample_token = sampling(y[0][sample_index])\n",
        "\n",
        "        tokens_generated.append(sample_token)\n",
        "        start_tokens.append(sample_token)\n",
        "\n",
        "    # Convert tokens back to text.\n",
        "    generated_text = tokenizer.decode(tokens_generated)\n",
        "    generated_text = generated_text.replace(tokenizer.decode([pad_token_id]), '')\n",
        "    return generated_text, probs\n",
        "\n",
        "\n",
        "def plot_next_token(probs_or_logits: np.ndarray,\n",
        "                    tokenizer: Any,\n",
        "                    prompt: str,\n",
        "                    keep_top: int = 30):\n",
        "    \"\"\"Plots the probability distribution of the next tokens.\n",
        "\n",
        "    This function generates a bar plot showing the top `keep_top`\n",
        "    tokens by probability.\n",
        "\n",
        "    # Function from Gemma\n",
        "    https://github.com/google-deepmind/gemma/blob/ee0d55674ecd0f921d39d22615e4e79bd49fce94/gemma/gm/text/_tokenizer.py#L249-L284\n",
        "\n",
        "    Args:\n",
        "        probs_or_logits: The raw logits output by the model or\n",
        "                         the probability distribution for the next token\n",
        "                         prediction.\n",
        "        tokenizer: The tokenizer used to decode token IDs to human-readable\n",
        "                   text.\n",
        "        prompt: The input prompt used to generate the next token predictions.\n",
        "        keep_top: The number of top tokens to display in the plot.\n",
        "                  Default is 30.\n",
        "\n",
        "    Returns:\n",
        "        None: Displays a plot showing the probability distribution of the\n",
        "              top tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    if np.isclose(probs_or_logits.sum(), 1):\n",
        "        probs = probs_or_logits\n",
        "    else:\n",
        "        # Apply softmax to logits to get probabilities\n",
        "        probs = jax.nn.softmax(probs_or_logits)\n",
        "\n",
        "    # Select the top `keep_top` tokens by probability\n",
        "    indices = jnp.argsort(probs)\n",
        "\n",
        "    # Reverse to get highest probabilities first\n",
        "    indices = indices[-keep_top:][::-1]\n",
        "\n",
        "    # Get the probabilities and corresponding tokens\n",
        "    probs = probs[indices].astype(np.float32)\n",
        "    tokens = [repr(tokenizer.decode(i.item())) for i in indices]\n",
        "\n",
        "    # Create the bar plot using Plotly.\n",
        "    fig = px.bar(x=tokens, y=probs)\n",
        "\n",
        "    # Customize the plot layout.\n",
        "    fig.update_layout(\n",
        "        title='Probability Distribution of Next '\n",
        "              f'Tokens given the prompt=\"{prompt}\"',\n",
        "        xaxis_title='Tokens',\n",
        "        yaxis_title='Probability',\n",
        "    )\n",
        "\n",
        "    # Display the plot.\n",
        "    fig.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "oZAHvAaAAvKI"
      },
      "cell_type": "markdown",
      "source": [
        "The `create_model` function used below constructs a transformer model, a potent neural network architecture widely employed in natural language processing. After creating the model, print out the summary of the model.\n",
        "\n",
        "\n",
        "\n",
        "- **`create_model(maxlen=maxlen, vocab_size=vocab_size)`**:\n",
        "    - This is a function that builds the transformer model.\n",
        "    - `maxlen` refers to the maximum length of the sequences that the transformer model will process.\n",
        "    - `vocab_size` refers to the size of the vocabulary (the total number of unique words or tokens the model can understand). This is used to determine the number of unique inputs the model should expect (for example, it could be the number of unique tokens in the text dataset).\n",
        "- **`model.summary()`**:\n",
        "    - This line prints out a summary of the model architecture.\n",
        "    - The summary will show a breakdown of the different layers in the model, the number of parameters each layer has, and the output shape of each layer. This summary is useful for understanding the structure of the model, like how data flows through it and where the parameters are adjusted during training.\n",
        "\n",
        "**What to look for in the summary:**\n",
        "\n",
        "- Layer names: The different layers of the model (like Input, Dense, etc.) will be listed.\n",
        "\n",
        "- Output shape: The shape of the data as it moves through each layer.\n",
        "\n",
        "- Parameters: How many parameters are in each layer. These are the numbers that will be adjusted during training to decrease loss."
      ]
    },
    {
      "metadata": {
        "id": "iNGqssFqlzqE"
      },
      "cell_type": "code",
      "source": [
        "model = create_model(maxlen=maxlen, vocab_size=tokenizer.vocab_size)\n",
        "print(model.summary())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "tVA4N5N9nAwB"
      },
      "cell_type": "markdown",
      "source": [
        "For monitoring progress, define a callback function that is used to regularly print the generated words during training. This function allows you to track the learning progress of the language model.  You can specify the number of words to print and the initial prompt to guide the model's generation:"
      ]
    },
    {
      "metadata": {
        "id": "1GhWqSRVm_ql",
        "collapsed": true
      },
      "cell_type": "code",
      "source": [
        "prompt = 'Abeni,'\n",
        "start_words = tokenizer.encode(prompt)\n",
        "text_gen_callback = TextGenerator(max_tokens=10, start_tokens=start_words, tokenizer=tokenizer)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1P_McuozGX6R"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell to train the model. The training process updates the model parameter after each step.\n",
        "\n",
        "- A step is one model update after learning on a batch of examples.\n",
        "\n",
        "- An epoch (`num_epoch`) is the number of times the model goes through the entire dataset.  `num_examples / batch_size` is the number of steps needed for one epoch.\n",
        "\n",
        "When training the model below, you will specify the number of epochs."
      ]
    },
    {
      "metadata": {
        "id": "mbgwJD9z83DF"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to train the model. Rerun it until you achieve a training loss of around 0.1.\n",
        "\n",
        "It is recommendend to train the model for at least `200` epochs. But if training is taking long, you can reduce the number of epochs:"
      ]
    },
    {
      "metadata": {
        "id": "HiJciyOo5gYU"
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 200  #@param {type: 'number'}\n",
        "# verbose=2: Instructs the model.fit method to print one line per\n",
        "# epoch so you see how the loss is decreasing and generated texts improving.\n",
        "history = model.fit(x=dataset, verbose=2,\n",
        "                    epochs=num_epochs,\n",
        "                    callbacks=[text_gen_callback])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "zf1BR0cGoy0V"
      },
      "cell_type": "markdown",
      "source": [
        "Now that you have a trained model, you can prompt it like you did in \"Lab 3 - Experiment with a Transformer Model\"."
      ]
    },
    {
      "metadata": {
        "id": "DTbezoZr5VKc"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 7: Prompting the trained model\n",
        "\n",
        "Now that you are going to prompt the SLM, in this section, you will ask the four following key questions to evaluate its quality. The questions are:\n",
        "\n",
        "*   A. How good is the SLM at predicting the next word of a given prompt (prior words) based on patterns identified in the training dataset?\n",
        "*   B. Is the generated text coherent, and does it make sense given the context?\n",
        "*   C. Is the likely next token what you expect to see when the context is changed slightly?\n",
        "*   D. How does the model handle unseen tokens?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ukARAmZpEUOJ"
      },
      "cell_type": "markdown",
      "source": [
        "**A. How good is the SLM at predicting the next word of a given prompt (prior words) based on patterns identified in the training dataset?**\n",
        "\n",
        "- Prompt the model using a word or sequence of words from the training dataset. For example, you can start with `'Abeni, a bright-eyed'`\n",
        "- Visualize the probability distribution of the next token of given prompt\n",
        "- Increase the `num_next_words` number to see more text.\n",
        "- Set `do_sample=False` to \"greedily\" pick the next token given the context (prior tokens).\n",
        "- Inspect the generated text. See how well the model has learned to generate text that reflects the patterns learned during training:"
      ]
    },
    {
      "metadata": {
        "id": "5E06Mp-wFC8q"
      },
      "cell_type": "code",
      "source": [
        "prompt = 'Abeni, a bright-eyed' #@param {type: 'string'}\n",
        "num_next_words = 10 #@param {type: 'number'}\n",
        "generated_text, probs = generate_text(prompt, num_next_words, model=model, tokenizer=tokenizer, pad_token_id=tokenizer.pad_token_id, do_sample=False)\n",
        "plot_next_token(probs[0], tokenizer, prompt=prompt)\n",
        "print('\\n')\n",
        "print('Generated Text:', generated_text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "RXz3Wu-10jlX"
      },
      "cell_type": "markdown",
      "source": [
        "**B. Is the generated text coherent, and does it make sense given the context?**\n",
        "- Prompt the model with words or a phrase of your choosing.\n",
        "- Increase the `num_next_words` number to see more texts.\n",
        "- Visualize the probability distribution of the next token for the given prompt\n",
        "- Set `do_sample=True` to sample words from the probability distribution of the next token given the context (prior tokens).\n",
        "- Inspect the quality of generated texts.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "J1riRfT3lbA8"
      },
      "cell_type": "code",
      "source": [
        "prompt = 'Jide was hungry so she went looking for' #@param {type: 'string'}\n",
        "num_next_words = 10 #@param {type: 'number'}\n",
        "generated_text, probs = generate_text(prompt, num_next_words, model=model, tokenizer=tokenizer, pad_token_id=tokenizer.pad_token_id, do_sample=True)\n",
        "plot_next_token(probs[0], tokenizer, prompt=prompt)\n",
        "print('\\n')\n",
        "print('Generated Text:', generated_text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "VEoUkDVFd_0M"
      },
      "cell_type": "markdown",
      "source": [
        "**C. Is the likely next token what you expect to see when the context is changed slightly?**\n",
        "- Change the context of the prompt slightly.\n",
        "- Visualize the probability distribution of the next token for the given prompt.\n",
        "- Increase the `num_next_words` number to see more texts.\n",
        "- Set `do_sample=True` to sample words from probability distribution of the next token given the context (prior tokens).\n",
        "- Inspect the quality of generated texts:"
      ]
    },
    {
      "metadata": {
        "id": "hHFdQf4Fdn9x"
      },
      "cell_type": "code",
      "source": [
        "prompt = 'Jide was thirsty so she went looking for' #@param {type: 'string'}\n",
        "num_next_words = 10 #@param {type: 'number'}\n",
        "generated_text, probs = generate_text(prompt, num_next_words, model=model, tokenizer=tokenizer, pad_token_id=tokenizer.pad_token_id, do_sample=True)\n",
        "plot_next_token(probs[0], tokenizer, prompt=prompt)\n",
        "print('\\n')\n",
        "print('Generated Text:', generated_text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5_57Uax00DlH"
      },
      "cell_type": "markdown",
      "source": [
        "**D. How does the model handle unseen tokens?**\n",
        "\n",
        "- Prompt the model with words that are not present in the training dataset, like `'photosynthesis'`.\n",
        "- Visualize the probability distribution of the next token for the given prompt.\n",
        "- Increase the `num_next_words` number to see more texts.\n",
        "- Inspect the generated texts:"
      ]
    },
    {
      "metadata": {
        "id": "sBBEIkiF0O9E"
      },
      "cell_type": "code",
      "source": [
        "prompt = 'Photosynthesis is the process ' #@param {type: 'string'}\n",
        "num_next_words = 10 #@param {type: 'number'}\n",
        "generated_text, probs = generate_text(prompt, num_next_words, model=model, tokenizer=tokenizer, pad_token_id=tokenizer.pad_token_id, do_sample=True)\n",
        "plot_next_token(probs[0], tokenizer, prompt=prompt)\n",
        "print('\\n')\n",
        "print('Generated Text:', generated_text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1XZcNdrE0Upb"
      },
      "cell_type": "markdown",
      "source": [
        "Did you notice how the model replaced the word `photosynthesis` with the unknown `'<UNK>'` token? The generated sentence has nothing to do with photosynthesis, since the dataset does not contain any information about this. The dataset used to train the model dictates the knowledge it can exhibit based on the patterns it has learned during training."
      ]
    },
    {
      "metadata": {
        "id": "lTIY3uK08yUn"
      },
      "cell_type": "markdown",
      "source": [
        "## Experimentation and hyperparameter tuning\n",
        "Above, you have used a few hyperparameters to train the SLM.\n",
        "\n",
        "**What are hyperparameters?**\n",
        "\n",
        "Hyperparameters are settings or values that you define before training a model. You can learn about them in detail [here](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)). They control the learning process. These are not learned by the model itself but are set by the user. In other words, hyperparameters help guide how the model learns from the data.\n",
        "\n",
        "- `batch_size`: The batch size determines how many samples are included in each batch for model update.\n",
        "\n",
        "- `num_epochs`: This is the number of times the model goes through the entire training dataset. An epoch consists of several iterations because the model goes through training data in batches. In each iteration, the model calculates loss and uses it to adjust its predictions. More epochs mean more times a model gets to adjust its understanding of the task and data. However, more epochs also means more time taken to train the model. This corresponds to `num_examples / batch_size` steps of training.\n",
        "\n",
        "\n",
        "Another hyperparameter you've seen is `maxlen`, which  refers to the maximum number of tokens that each sequence will be padded or truncated to."
      ]
    },
    {
      "metadata": {
        "id": "W3x9IlUjDraW"
      },
      "cell_type": "markdown",
      "source": [
        "Now, adjust some of these hyperparameters and compare the results:\n",
        "\n",
        "Your tasks:\n",
        "\n",
        "1. Adjust the `batch_size` and `num_epochs` hyperparameters. Try out different configurations. For example, you can try different batch sizes (e.g, 8, 16, 32, 64...) and different numbers of epochs (e.g, 5, 10, 20...).\n",
        "\n",
        "2. Run the model for each configuration.\n",
        "\n",
        "3. Create a table to write down the loss at the end.\n",
        "\n",
        "Below is an example of the table you should create.\n",
        "\n",
        "\n",
        "|batch_size | num_epochs | train_loss |\n",
        "|---|---|---|\n",
        "|8 | 5 | 6.91|\n"
      ]
    },
    {
      "metadata": {
        "id": "FL_EDmj2qnYI"
      },
      "cell_type": "markdown",
      "source": [
        "Change the `batch_size` and `num_epochs`. Then, run the cell below to train the model:"
      ]
    },
    {
      "metadata": {
        "id": "gr1eCmHpn5UO"
      },
      "cell_type": "code",
      "source": [
        "# Prepare the dataset for training.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input, target))\n",
        "\n",
        "# Shuffle the examples in the dataset.\n",
        "dataset = dataset.shuffle(buffer_size=len(input))\n",
        "\n",
        "# Specify the batch size.\n",
        "batch_size = 8  #@param {type: 'number'}\n",
        "\n",
        "# Create batches of examples used to update the model parameters.\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "model = create_model(maxlen=maxlen, vocab_size=tokenizer.vocab_size)\n",
        "\n",
        "# Specify the number of epochs to train the model for.\n",
        "num_epochs = 5  #@param {type: 'number'}\n",
        "\n",
        "# Train the model.\n",
        "history = model.fit(x=dataset, verbose=2,\n",
        "                    epochs=num_epochs)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "qGmGKUsc8U66"
      },
      "cell_type": "markdown",
      "source": [
        "> Are you running into an \"Out of Memory\" error?\n",
        "\n",
        "If you're getting an \"Out of Memory\" error, it means your system doesn't have enough memory to process the data. Here are some practical solutions:\n",
        "\n",
        "Consider trying the following:\n",
        "\n",
        "1. Reduce the `maxlen`.\n",
        "\n",
        "    Lower the number of words (tokens) processed at once. Shorter sequences need less memory. Consider truncating long sequences to a smaller length.\n",
        "\n",
        "2. Reduce the `batch_size`.\n",
        "\n",
        "    A smaller batch means less data is processed at once, reducing memory requirements."
      ]
    },
    {
      "metadata": {
        "id": "HHrgLwtmoY2w"
      },
      "cell_type": "markdown",
      "source": [
        "## Reflection\n",
        "\n",
        "This is the end of Lab 5 - Training Your Own Small Language Model (SLM).\n",
        "\n",
        "In this lab, you trained your first SLM and engaged in the following steps.\n",
        "\n",
        "- **Tokenized the dataset:** You used the `SimpleWordTokenizer` from the previous lab to convert the text descriptions into numerical representations.\n",
        "\n",
        "- **Padded the sequences:** You ensured all sequences have the same length by padding them with a special `'<PAD>'` token.  This is crucial for processing data in neural networks.\n",
        "\n",
        "- **Prepared the input and target data:** You created input-target pairs, where the target is the input sequence shifted by one token.  This teaches the model to predict the next word based on the context (prior words).\n",
        "\n",
        "- **Shuffled and batched the data:** You shuffled the dataset to randomize the training examples and grouped them into batches for efficient processing.\n",
        "\n",
        "- **Trained the SLM:** You built and trained a small transformer model, observing how the training loss decreased over epochs.\n",
        "\n",
        "- **Prompted the trained model:** You experimented with prompting the model, observing its ability to predict likely next word, generate coherent text, handle unseen words (represented by `'<UNK>'`), and adapt to changes in context.\n",
        "\n",
        "- **Experimented with hyperparameters:** You learned about hyperparameters, such as `batch_size`, `num_epochs`, and explored their impact on training by trying different values.\n",
        "\n",
        "\n",
        "The next section of the course delves deeper into model evaluation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
