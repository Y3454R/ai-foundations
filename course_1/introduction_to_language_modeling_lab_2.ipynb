{
  "cells": [
    {
      "metadata": {
        "id": "VlsqLYX5SpeK"
      },
      "cell_type": "markdown",
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "metadata": {
        "id": "XnknBbcaGpMK"
      },
      "cell_type": "markdown",
      "source": [
        "# **Build Your Own Small Language Model, Lab 2: Experiment with N-gram Models**\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/introduction_to_language_modeling_lab_2.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, you will build an n-gram model. An n-gram is a contiguous sequence of *n* words. An n-gram model uses these sequences to predict the probability of the next word given a preceding sequence of n-1 words (the context). The process is broken down into steps for you to follow."
      ],
      "metadata": {
        "id": "NwUnpnvIivF4"
      }
    },
    {
      "metadata": {
        "id": "1rTj9QNCr2NC"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1: Define dataset and break sentences into individual words\n",
        "\n",
        "First, define the *corpus* (text) used, which in this exercise will be [AfricaGalore](https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json). This dataset comprises of synthetically generated paragraphs:"
      ]
    },
    {
      "metadata": {
        "id": "r8pisvnSgRPD"
      },
      "cell_type": "code",
      "source": [
        "# Packages used.\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "avAcLDAXPE2o"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to load the dataset:"
      ]
    },
    {
      "metadata": {
        "id": "Pa22xutOAiTx"
      },
      "cell_type": "code",
      "source": [
        "africa_galore = pd.read_json('https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json')\n",
        "train_dataset = africa_galore['description']\n",
        "print(f'The dataset consists of {train_dataset.shape[0]} paragraphs.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "xUt0FYhXKg_N"
      },
      "cell_type": "markdown",
      "source": [
        "Now inspect the first ten paragraphs:"
      ]
    },
    {
      "metadata": {
        "id": "IRYzvOkxCILc"
      },
      "cell_type": "code",
      "source": [
        "for paragraph in train_dataset[:10]:\n",
        "    # `repr` returns a printable representational string.\n",
        "    print(repr(paragraph), '\\n')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "q-wtFwrL5_gn"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "In order to create n-grams, sentences must be converted into individual words. This is also called  *tokenization* . The simplest way to tokenize is to break sentences into individual units based on whitespace. When you tokenize the sentence `'Bimpe didn't come home yesterday'`, you'll get a list of tokens like `['Bimpe', \"didn't\", 'come', 'home', 'yesterday']`:\n",
        "\n",
        "The cell below creates a `split_text` function to split the string of text into a list of tokens."
      ]
    },
    {
      "metadata": {
        "id": "mns1I5yzx-kp"
      },
      "cell_type": "code",
      "source": [
        "def split_text(text: str) -> list[str]:\n",
        "    \"\"\"Splits a string into a list of words (tokens).\n",
        "\n",
        "    Splits text on whitespace.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "\n",
        "    Returns:\n",
        "        A list of tokens. Returns empty list if text is empty or all whitespace.\n",
        "    \"\"\"\n",
        "    words = text.split(' ')\n",
        "    return words\n",
        "\n",
        "# Tokenize an example text with the `split_text` function.\n",
        "split_text('Here\\'s how you tokenize! Awesome, isn\\'t it?')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "j_9Ns7sUB-VT"
      },
      "cell_type": "markdown",
      "source": [
        "This tokenizer is extremely naive. When splitting text based on whitespace alone, you will observe that the punctuation marks are now part of the token. For example, `'tokenize!'` will be a different token from `'tokenize'`. Although this function has limitations, it is still important to explore it in more detail."
      ]
    },
    {
      "metadata": {
        "id": "ni2mxLCfY-SK"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to tokenize the first paragraph in the dataset. The same tokenizer, which creates tokens based on whitespace, will be used:"
      ]
    },
    {
      "metadata": {
        "id": "Xno1DO0sY87o"
      },
      "cell_type": "code",
      "source": [
        "print(split_text(train_dataset[0]))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "wMiC7Jq0YtXq"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Create n-grams out of words\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iN5rFa4IQLi2"
      },
      "cell_type": "markdown",
      "source": [
        "The next step is to examine the n-grams in the data. Remember, n-grams are a collection of words. For example, a unigram is one word, a bigram is two words, a trigram is three words, and so on:"
      ]
    },
    {
      "metadata": {
        "id": "1XSmAIeuSc5r"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to generate the n-grams:"
      ]
    },
    {
      "metadata": {
        "id": "IJx9P17ox-kp"
      },
      "cell_type": "code",
      "source": [
        "all_unigrams=[]\n",
        "all_bigrams=[]\n",
        "all_trigrams=[]\n",
        "\n",
        "def generate_ngrams(text: str, n: int) -> list[str]:\n",
        "    \"\"\"Generates n-grams from a given text.\n",
        "\n",
        "    Args:\n",
        "        text: The input text string.\n",
        "        n: The size of the n-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
        "\n",
        "    Returns:\n",
        "        A list of n-gram strings.\n",
        "    \"\"\"\n",
        "\n",
        "    words = split_text(text)\n",
        "    ngrams = [' '.join(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "\n",
        "for paragraph in train_dataset:\n",
        "    all_unigrams.extend(generate_ngrams(paragraph, n=1)) # n=1 means a unigram.\n",
        "    all_bigrams.extend(generate_ngrams(paragraph, n=2))  # n=2 means a bigram.\n",
        "    all_trigrams.extend(generate_ngrams(paragraph, n=3)) # n=3 means a trigram.\n",
        "\n",
        "\n",
        "print('First 10 Unigrams:', all_unigrams[:10])\n",
        "print('First 10 Bigrams:', all_bigrams[:10])\n",
        "print('First 10 Trigrams:', all_trigrams[:10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "esDTLIxaHmLA"
      },
      "cell_type": "markdown",
      "source": [
        "In this cell, we will inspect which n-grams appear most frequently in the dataset.\n",
        "The output of the cell below is a list with a collection of *tuples*. You can read more about tuples [in this link](https://www.w3schools.com/python/python_tuples.asp). The output of the cell is in this format: `tuple(ngram, number of occurrences)`. For example, the bigram `'is a'` appears 134 times in our dataset:"
      ]
    },
    {
      "metadata": {
        "id": "2azMP6I4x-kp"
      },
      "cell_type": "code",
      "source": [
        "bigram_counts = Counter(all_bigrams)\n",
        "print('Most common Bigrams')\n",
        "print(bigram_counts.most_common(10))\n",
        "\n",
        "print('Most common Trigrams')\n",
        "trigram_counts=Counter(all_trigrams)\n",
        "print(trigram_counts.most_common(10))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "tr5wOBnVx-kp"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `generate_ngrams` function defined above can now be used to get the counts of all possible n-grams in the data. This gives the numerator of the conditional probability formula: $$ \\text{Count(A B)}$$"
      ]
    },
    {
      "metadata": {
        "id": "VZaL5Lu4x-kp"
      },
      "cell_type": "code",
      "source": [
        "def get_ngram_counts(train_dataset: list[str],\n",
        "                     n: int) -> tuple[dict[str, Counter], pd.DataFrame]:\n",
        "    \"\"\"Generates an n-gram count matrix and a dataframe from a training dataset.\n",
        "\n",
        "    This function takes a list of text strings (paragraphs or sentences) as\n",
        "    input, generates n-grams from each text, and creates a matrix where:\n",
        "\n",
        "    * Columns represent n-1 word contexts.\n",
        "    * Rows represent possible next words (the nth word).\n",
        "    * Cells contain the count of how often a next word follows a given context.\n",
        "\n",
        "    Args:\n",
        "        train_dataset: A list of text strings representing the training data.\n",
        "        n: The size of the n-grams to generate (e.g., 2 for bigrams, 3 for\n",
        "            trigrams).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A dictionary where keys are (n-1)-word contexts and values are Counter\n",
        "          objects storing the counts of each next word for that context.\n",
        "        - A Pandas DataFrame representing the n-gram count matrix, with contexts\n",
        "          as rows, next words as columns, and counts as cell values. Missing\n",
        "          values are filled with 0.\n",
        "    \"\"\"\n",
        "    ngram_counts = defaultdict(Counter)\n",
        "\n",
        "    for paragraph in train_dataset:\n",
        "        for ngram in generate_ngrams(paragraph, n):\n",
        "            words = split_text(ngram)\n",
        "            context = ' '.join(words[:-1])\n",
        "            next_word = words[-1]\n",
        "            ngram_counts[context][next_word] += 1\n",
        "\n",
        "    matrix = {context: dict(ngram_counts[context]) for context in ngram_counts}\n",
        "\n",
        "    # Convert to DataFrame and fill missing values with 0.\n",
        "    df = pd.DataFrame(matrix).fillna(0).astype(int)\n",
        "\n",
        "    return matrix, df\n",
        "\n",
        "# Example usage of the function.\n",
        "sample_data = ['This is a sample sentence.',\n",
        "               'Another sample sentence.',\n",
        "               'Split a sentence.']\n",
        "ngram_counts, df = get_ngram_counts(sample_data, 2)\n",
        "\n",
        "print('N-gram Counts Dictionary:')\n",
        "print(ngram_counts)\n",
        "\n",
        "print('\\nPandas DataFrame:')\n",
        "print(df)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "MjHpBNaNknJQ"
      },
      "cell_type": "markdown",
      "source": [
        "The `Count('sample sentence.')` is 2, since the bigram `'sample sentence.'` appears twice.\n",
        "In the following step, apply this function to the training dataset and explore bigram and trigram counts.\n"
      ]
    },
    {
      "metadata": {
        "id": "yOC1Lkbb_QuO"
      },
      "cell_type": "markdown",
      "source": [
        "Now, run the cell below to investigate some bigram counts."
      ]
    },
    {
      "metadata": {
        "id": "qRHDd6pTkBY8"
      },
      "cell_type": "code",
      "source": [
        "bigram_counts, bigram_df = get_ngram_counts(train_dataset, n=2)\n",
        "bigram_df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "rqbmtcWNmdbK"
      },
      "cell_type": "markdown",
      "source": [
        "In this bigram count matrix, each column represents the context (n-1) and each row represents the count of a possible next word. Because language can form an enormous number of potential word combinations, most of these combinations (5347 x 5347 as seen above) never occur (e.g., `'resonates refried'`) or occur very rarely (e.g., `'that a'`). As a result, most of the entries in this matrix are zero. This phenomenon, known as *sparsity*, indicates that, while the matrix may have thousands of cells, only a small fraction contain non-zero counts.\n",
        "\n",
        "The following cell will explore bigrams in the dataset starting with the word `'name'`:"
      ]
    },
    {
      "metadata": {
        "id": "jy84xgPUlbmq"
      },
      "cell_type": "code",
      "source": [
        "bigram_counts['name']"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "luMFOggul7w_"
      },
      "cell_type": "markdown",
      "source": [
        "This shows the following bigram counts:\n",
        "\n",
        "`'name that'`: 2\n",
        "\n",
        "`'name in'`: 1\n",
        "\n",
        "`'name from'`: 1\n",
        "\n",
        "`'name means'`: 2\n",
        "\n",
        "It can therefore be concluded that `'name'` occurs 2+1+1+2 = 6 times in the dataset:"
      ]
    },
    {
      "metadata": {
        "id": "-bO-WIBb_Wzl"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to inspect trigram counts:"
      ]
    },
    {
      "metadata": {
        "id": "blX_r7XXlJON"
      },
      "cell_type": "code",
      "source": [
        "trigram_counts, trigram_df = get_ngram_counts(train_dataset, n=3)\n",
        "trigram_df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "SVF46J_Jmp2o"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below to check the word that follows the bigram `'a name'`:\n"
      ]
    },
    {
      "metadata": {
        "id": "-zOoq3gFjsq9"
      },
      "cell_type": "code",
      "source": [
        "trigram_counts['a name']"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5AgcdtOxZ5Yb"
      },
      "cell_type": "markdown",
      "source": [
        "This means that the bigram `'a name'` is followed only by the word `'that'` and the trigram `'a name that'`, which appears twice in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "76CwEsOfsHv2"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Calculate Count(A)\n",
        "\n",
        "Recall the conditional probability formula:\n",
        "\n",
        "$$ P(\\text{B} \\mid \\text{A}) = \\frac{\\text{Count(A B)}}{\\text{Count(A)}} $$\n",
        "\n",
        "As a third step, calculate $${\\text{Count(A)}}$$\n",
        "\n",
        "Based on this formula, dividing these n-gram counts by the total count of words in context (step 3) will give the desired conditional probabilities."
      ]
    },
    {
      "metadata": {
        "id": "isbFiZ2EyyO4"
      },
      "cell_type": "code",
      "source": [
        "def build_ngram_model(train_dataset: list[str], n: int) -> dict[str, dict[str, float]]:\n",
        "    \"\"\"Builds an n-gram language model.\n",
        "\n",
        "    This function takes a list of text strings (paragraphs or sentences) as\n",
        "    input, generates n-grams from each text using the function get_ngram_counts\n",
        "    and converts them into probabilities.  The resulting model is a dictionary,\n",
        "    where keys are (n-1)-word contexts and values are dictionaries mapping\n",
        "    possible next words to their conditional probabilities given the context.\n",
        "\n",
        "    Args:\n",
        "        train_dataset: A list of text strings representing the training data.\n",
        "        n: The size of the n-grams (e.g., 2 for a bigram model).  This argument\n",
        "            is not used directly in the calculation but is included for clarity\n",
        "            and consistency.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the n-gram language model, where keys are\n",
        "        (n-1)-word contexts and values are dictionaries mapping possible next\n",
        "        words to their conditional probabilities.\n",
        "    \"\"\"\n",
        "    ngram_model = {}\n",
        "\n",
        "    ngram_counts, _ = get_ngram_counts(train_dataset, n)\n",
        "\n",
        "    for context, next_words in ngram_counts.items():\n",
        "        context_total_count = sum(next_words.values())\n",
        "        ngram_model[context] = {\n",
        "            word: count / context_total_count for word, count in next_words.items()\n",
        "        }\n",
        "\n",
        "    return ngram_model"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "wm0g-BMtbTgK"
      },
      "cell_type": "markdown",
      "source": [
        "The example used in the previous step is useful when understanding the code above.\n",
        "\n",
        "The context in this case is `name`.\n",
        "\n",
        "Variable context_total_count is the number of times `name` occurs, i.e. 2+1+1+2 = 6.\n",
        "\n",
        "Variable next_words is `{'that': 2, 'in': 1, 'from': 1, 'means': 2}`\n",
        "\n",
        "Based on the conditional probability formula, you should expect the following probabilities:\n",
        "\n",
        "`'name that'`: 2 / 6 = 0.33\n",
        "\n",
        "`'name in'`: 1 / 6 = 0.167\n",
        "\n",
        "`'name from'`: 1 / 6 = 0.167\n",
        "\n",
        "`'name means'`: 2 / 6 = 0.33\n",
        "\n",
        "A bigram model can be created to verify these probabilities:"
      ]
    },
    {
      "metadata": {
        "id": "rLQTEf29qe7Y"
      },
      "cell_type": "markdown",
      "source": [
        "## Bigram model\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QDQ9nD3QcNdT"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using the model for the context `'name'` results in another dictionary with candidate words as keys and their conditional probabilities as values:"
      ]
    },
    {
      "metadata": {
        "id": "tv3KTRzhoT4-"
      },
      "cell_type": "code",
      "source": [
        "bigram_model = build_ngram_model(train_dataset, n=2)\n",
        "\n",
        "bigram_model['name']"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3UeuxnPjjRxc"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the n-gram model probabilities above match the expectations.\n"
      ]
    },
    {
      "metadata": {
        "id": "_vTjwzLPqXgW"
      },
      "cell_type": "markdown",
      "source": [
        "### Understanding the output of the model"
      ]
    },
    {
      "metadata": {
        "id": "tu2h5vcYhOG-"
      },
      "cell_type": "markdown",
      "source": [
        "If you are unclear about what happened in the cell above, it is important to understand the datatype of the model. This n-gram model is a dictionary where:\n",
        "\n",
        "- Keys: Represent the context (for example, a unigram `'for'`, or bigram `'looking for'`).\n",
        "\n",
        "- Values: Represent another dictionary. Its keys are all candidates words observed in the dataset, and its values are their probabilities.\n",
        "\n",
        "Below is an example of a dictionary within a dictionary:\n"
      ]
    },
    {
      "metadata": {
        "id": "VnIIWTRrY8CA"
      },
      "cell_type": "code",
      "source": [
        "# Example: A dictionary of student grades.\n",
        "student_grades = {\n",
        "    'Annie': {\n",
        "        'Math': 95,\n",
        "        'Science': 88,\n",
        "        'History': 92\n",
        "    },\n",
        "    'Teju': {\n",
        "        'Math': 85,\n",
        "        'Science': 90,\n",
        "        'History': 80\n",
        "    }\n",
        "}\n",
        "\n",
        "# Accessing the dictionary for Teju.\n",
        "teju_grades = student_grades['Teju']\n",
        "print('Teju\\'s grades:', teju_grades)\n",
        "\n",
        "# Extracting the subjects (keys) for Teju.\n",
        "subjects = list(teju_grades.keys())\n",
        "print('Subjects for Teju:', subjects)\n",
        "\n",
        "# Extracting the grades (values) for Teju.\n",
        "grades = list(teju_grades.values())\n",
        "print('Grades for Teju:', grades)\n",
        "\n",
        "# Extracting the Math grades for Teju.\n",
        "math_grades = teju_grades['Math']\n",
        "print('Math Grade for Teju', math_grades)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "EuXP7DaaqcIC"
      },
      "cell_type": "markdown",
      "source": [
        "## Trigram model\n",
        "This step will create a trigram model and examine some probabilities:"
      ]
    },
    {
      "metadata": {
        "id": "wLfHXmpeqNDR"
      },
      "cell_type": "code",
      "source": [
        "trigram_model = build_ngram_model(train_dataset, n=3)\n",
        "sample_keys = list(trigram_model.keys())[:5]\n",
        "for key in sample_keys:\n",
        "    print(f'{key}: {trigram_model[key]}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "QeA6fZW7tg5J"
      },
      "cell_type": "markdown",
      "source": [
        "The probabilities of candidate words following the bigram `'a name'` are shown below:"
      ]
    },
    {
      "metadata": {
        "id": "Dv_BNhzZrU9z"
      },
      "cell_type": "code",
      "source": [
        "trigram_model['a name']"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "VqioqxIVrpGs"
      },
      "cell_type": "markdown",
      "source": [
        "This means that the trigram `'a name that'` appears once in the dataset. That means the bigram `'a name'` is always followed by the word `'that'`.\n",
        "\n",
        "What results would searching for  `'The name'` yield?"
      ]
    },
    {
      "metadata": {
        "id": "8RDyWq24roG2"
      },
      "cell_type": "code",
      "source": [
        "trigram_model['The name']"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Uli9Gqy9r2un"
      },
      "cell_type": "markdown",
      "source": [
        "How about `'Their name'`?"
      ]
    },
    {
      "metadata": {
        "id": "S6D9iLQNr628"
      },
      "cell_type": "code",
      "source": [
        "trigram_model['Their name']"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "pAchRjq8r8Wb"
      },
      "cell_type": "markdown",
      "source": [
        "This creates an error. The reason is that the bigram `'Their name'` does not exist in the dataset. Therefore, it cannot be provided as context for the trigram model.\n"
      ]
    },
    {
      "metadata": {
        "id": "py8WlM2ARNhZ"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding exercise\n",
        "\n",
        "Now that you have a model, you need code to predict the next word based on a given context. You will reuse `random.choices` from the previous lab, where it randomly selected a candidate word from a list based on specified weights (probabilities). In the previous lab, the probabilities came from your mental model of English. This time, the probabilities will come from the trigram model. As a refresher, see a sample usage of `random.choices` below.\n"
      ]
    },
    {
      "metadata": {
        "id": "YrXfivl8SKyV"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below multiple times to see different candidate words being picked:"
      ]
    },
    {
      "metadata": {
        "id": "qNHq3vS1Rj8m"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define a list of items.\n",
        "sample_candidate_words = ['apple', 'banana', 'cherry']\n",
        "\n",
        "# Define corresponding weights for each fruit.\n",
        "# The probability of choosing each fruit is proportional to its weight.\n",
        "weights = [0.2, 0.5, 0.3]\n",
        "\n",
        "# Sample one fruit based on the weights.\n",
        "# The 'k=1' parameter indicates that we're selecting one item.\n",
        "chosen_fruit = random.choices(sample_candidate_words, weights=weights, k=1)[0]\n",
        "\n",
        "print('Chosen fruit:', chosen_fruit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "CzOxiLBJI_Hv"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "To use random choice for the context `'looking for'`, you need to write code that performs the following steps:\n",
        "\n",
        "1. Extract candidate words for a given context from the trigram_model using the `.keys()` method.\n",
        "\n",
        "2. Extract the corresponding probability weights using the `.values()` method."
      ]
    },
    {
      "metadata": {
        "id": "Ba69l0KSVcFX"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 1.** Extract candidate words for a given context from the trigram_model using the `.keys()` method"
      ]
    },
    {
      "metadata": {
        "id": "zBgdyD0XVa3A"
      },
      "cell_type": "code",
      "source": [
        "context = 'looking for'\n",
        "candidate_words = # Enter code here.\n",
        "candidate_words"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "vUoxsVyS7xmn",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "# @title Run this to test your code, or get a hint.\n",
        "def test_candidate_words():\n",
        "    hint = \"\"\"\n",
        "          Hint:\n",
        "          ======\n",
        "          The ngram model is a dictionary where each context maps to another\n",
        "          dictionary of candidate words.\n",
        "          To get a list of candidate words for a specific context, put the\n",
        "          context in the model as ngram_model[context] and then use the .keys()\n",
        "          method.\n",
        "          \"\"\"\n",
        "\n",
        "    context = 'looking for'\n",
        "\n",
        "    try:\n",
        "        if candidate_words == trigram_model[context].keys():\n",
        "            print('Nice! Your answer looks correct.')\n",
        "        else:\n",
        "            print('\\033[1m\\033[91mSorry, your answer is not correct.\\033[0m')\n",
        "            give_hints = input('Would you like a hint? Type Yes or No.')\n",
        "            if give_hints.lower() in ['yes', 'y']:\n",
        "                print(hint)\n",
        "    except NameError:\n",
        "        print('Hint: The variable \\'candidate_words\\' is not defined.')\n",
        "        print(f'You need to write some code after candidate_words={candidate_words}')\n",
        "\n",
        "test_candidate_words()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "NKbYdOfmVlZD"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 2.** Extract the corresponding probability weights using the `.values()` method."
      ]
    },
    {
      "metadata": {
        "id": "V2WHQjstVnQQ"
      },
      "cell_type": "code",
      "source": [
        "context = 'looking for'\n",
        "weights = # Enter code here.\n",
        "weights"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "oKZlBxziX13g",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "# @title Run this to test your code, or get a hint.\n",
        "def test_weights():\n",
        "    hint = \"\"\"\n",
        "          Hint:\n",
        "          ======\n",
        "          This is the same as the step above except this time around we are\n",
        "          fetching the values, not keys. Try using .values() instead of .keys().\n",
        "          \"\"\"\n",
        "\n",
        "    try:\n",
        "        if list(weights) == list(trigram_model[context].values()):\n",
        "            print('Nice! Your answer looks correct.')\n",
        "        else:\n",
        "            print('\\033[1m\\033[91mSorry, your answer is not correct.\\033[0m')\n",
        "            give_hints = input('Would you like a hint? Type Yes or No.')\n",
        "            if give_hints.lower() in ['yes', 'y']:\n",
        "                print(hint)\n",
        "    except:\n",
        "        print('Hint: The variable \\'weights\\' is not defined properly')\n",
        "        print(f'You need to write some code after weights={weights}')\n",
        "\n",
        "test_weights()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2Iq1yH1kVvHQ"
      },
      "cell_type": "markdown",
      "source": [
        "Put the candidate words and their weights in random.choices, and generate a next possible word for the context `'looking for'`.\n",
        "\n",
        "Run the cell below multiple times to see different words being picked:"
      ]
    },
    {
      "metadata": {
        "id": "6UOC_uQBVv8L"
      },
      "cell_type": "code",
      "source": [
        "next_word = random.choices(list(candidate_words), weights=weights)[0]\n",
        "print(context, next_word)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "oCoGHege65Z1"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate text\n",
        "\n",
        "Now that you have a probability distribution over n-grams, it can be used to generate the next words given a prompt.\n",
        "A prompt can be any phrase or word you want to give the model as a context or a starting point. A model can generate language by randomly selecting the next word based on their probabilities conditioned on preceding words (context).\n",
        "\n",
        "Text generation using an n-gram model is an iterative process where each newly generated word is added to the existing context. This forms the basis for predicting the next word.\n",
        "\n",
        "Starting with an initial prompt text, the model uses the probability distribution derived from the n-gram counts to select the most likely next word. The `random.choices` method will be used again to pick the next possible word. Once this word is generated, it is added to the context and the updated sequence is used to calculate the next probability distribution. This chain-like process continues until the next num_next_words number of words is generated, allowing the model to generate text by progressively expanding the sequence one word at a time:"
      ]
    },
    {
      "metadata": {
        "id": "uenGa3b4iHBo"
      },
      "cell_type": "code",
      "source": [
        "def generate_next_n_words(n: int, ngram_model: dict[str, dict[str, float]],\n",
        "                          prompt: str, num_next_words: int) -> str:\n",
        "    \"\"\"Generates the next 'num_next_words' words following a given prompt using\n",
        "    an n-gram language model.\n",
        "\n",
        "    This function takes an n-gram model and uses\n",
        "    it to predict the most likely next words given the provided prompt. The\n",
        "    generation process continues iteratively, appending predicted words to the\n",
        "    prompt until the desired number of words is generated or a context is\n",
        "    encountered for which the model has no predictions.\n",
        "\n",
        "    Args:\n",
        "        n: The size of the n-grams to use (e.g., 2 for a bigram model).\n",
        "        ngram_model: A dictionary representing the n-gram language model.\n",
        "        prompt: The starting text prompt for generating the next words.\n",
        "        num_next_words: The number of words to generate following the prompt.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the original prompt followed by the generated words.\n",
        "        If no valid continuation is found for a given context, the function will\n",
        "        return the text generated up to that point and print a message\n",
        "        indicating that no continuation could be found.\n",
        "    \"\"\"\n",
        "\n",
        "    generated_words = split_text(prompt)  # Split prompt into individual words.\n",
        "\n",
        "    for _ in range(num_next_words):\n",
        "        context = generated_words[-(n - 1):]  # Get last (n-1) words as context.\n",
        "        context = ' '.join(context)\n",
        "        if context in ngram_model:\n",
        "            # Sample next word based on probabilities.\n",
        "            next_word = random.choices(\n",
        "                list(ngram_model[context].keys()),\n",
        "                weights=ngram_model[context].values()\n",
        "            )[0]\n",
        "\n",
        "            generated_words.append(next_word)\n",
        "        else:\n",
        "            print('No valid continuation found. Change the prompt or '\n",
        "                  'try a smaller ngram')\n",
        "            break\n",
        "\n",
        "    return ' '.join(generated_words)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "SGJwKbsnIOXy"
      },
      "cell_type": "markdown",
      "source": [
        "#### Finish the prompt using a bigram model\n",
        "\n",
        "1. Run the following cell multiple times to see different results using a bigram model.\n",
        "2. Try a simpler prompt, preferably one that uses phrases from the training data.\n",
        "3. Try different values of n. **bold text**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xndDil588L39"
      },
      "cell_type": "code",
      "source": [
        "prompt = 'Jide was hungry so she went looking for'\n",
        "\n",
        "n = 2 # Bigram.\n",
        "num_next_words = 10 # Generate next n words.\n",
        "generate_next_n_words(n=n,\n",
        "                      ngram_model=bigram_model,\n",
        "                      prompt=prompt,\n",
        "                      num_next_words=num_next_words)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "AMZCFKMaISrV"
      },
      "cell_type": "markdown",
      "source": [
        "#### Finish the prompt using a trigram model\n",
        "\n",
        "1. Run the following cell multiple times to see different results using a trigram model.\n",
        "2. Try a simpler prompt, preferably one that uses phrases from the training data.\n",
        "3. Try different values of n."
      ]
    },
    {
      "metadata": {
        "id": "0Pq0FW_2IULl"
      },
      "cell_type": "code",
      "source": [
        "prompt = 'Jide was hungry so she went looking for'\n",
        "\n",
        "n = 3 # Trigram.\n",
        "num_next_words = 10 # Generate next n words.\n",
        "generate_next_n_words(n=n,\n",
        "                      ngram_model=trigram_model,\n",
        "                      prompt=prompt,\n",
        "                      num_next_words=num_next_words)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Oz0MoqxB96S_"
      },
      "cell_type": "markdown",
      "source": [
        "The different results when running the cell multiple times are because the n-gram model picks the next word from a probability distribution randomly (also called *sampling*). More probable next words have a high probability of getting picked but are not guaranteed to be picked. This gives the output its stochasticity. Stochasticity means randomness or chance."
      ]
    },
    {
      "metadata": {
        "id": "e5B0zuHWkGZr"
      },
      "cell_type": "markdown",
      "source": [
        "## What happens when you increase the *n* in n-grams?\n",
        "\n",
        "While it intuitively seems that a larger context (higher n) would lead to better quality output by capturing more long-range dependencies in language,  it quickly runs into the problem of data sparsity, that is high dimensional array with many zero values.\n",
        "\n",
        "When moving from bigrams (pairs of words) to trigrams (triplets of words), the number of possible combinations increases exponentially, and many of these triplets rarely, if ever, appear in the training data. This means that while bigram models can cover a significant portion of common word pairs, the majority of potential word sequences in trigram and higher-order models are underrepresented. This makes it more challenging for the model to reliably predict the next word.\n",
        "\n",
        "Consider a simple vocabulary of five words: `['I', 'love', 'to', 'eat', 'jollof']`. For bigrams, there are at most $$5 \\times 5 = 25 $$ possible combinations. In reality, however, your training data might only include common pairs like `'I love'`, `'love to'`, `'to eat'`, and `'eat jollof'`. Now, when you move to trigrams (triplets of words), there are $$ 5 \\times 5 \\times 5 = 125$$ possible combinations. However, only a few of these - such as `'I love to'`, `'love to eat'`, and `'to eat jollof'`, will actually appear in the data.\n",
        "\n",
        "Even with massive datasets, many of the higher-order n-grams will never appear in the training corpus. This results in many zero counts for the probabilities. As the n-gram order increases, the number of potential combinations grows exponentially. This often leads to many combinations being rare or absent in the training data. This way, making reliable probability estimation becomes more difficult.\n",
        "\n",
        "This phenomenon can be verified in the dataset by looking at the dimensions of the bigram matrix versus trigram matrix:"
      ]
    },
    {
      "metadata": {
        "id": "Lt9gY0ErOo-l"
      },
      "cell_type": "code",
      "source": [
        "print('The dimensions of our bigram matrix is', bigram_df.shape)\n",
        "print('The dimensions of our trigram matrix is', trigram_df.shape)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "RqOIH4XKt5ny"
      },
      "cell_type": "markdown",
      "source": [
        "## Reflection\n",
        "\n",
        "This is the end of **Lab 2: Experiment with N-gram Models**.\n",
        "\n",
        "This lab provided a hands-on exploration of n-gram models for language modeling. Here are some key takeaways and reflections:\n",
        "\n",
        "**1. Functionality:**\n",
        "\n",
        "- You saw how n-gram models can be used to predict the next word in a sequence based on the preceding words (context).\n",
        "- N-gram models are relatively simple to implement using the conditional probabilities formula to sample the next word.\n",
        "\n",
        "**2. Data sparsity:**\n",
        "\n",
        "- Data sparsity is a major challenge for n-gram models, especially with higher-order n-grams (trigrams or larger).\n",
        "- This sparsity arises because many possible word combinations are rare or absent in real-world text data.\n",
        "- You observed this in the dataset. The dimensions of the trigram matrix are significantly larger than those of the bigram matrix, resulting in more zero values.\n",
        "\n",
        "**3. Stochasticity and text generation:**\n",
        "\n",
        "- While the model assigns probabilities to different next words, the actual choice is stochastic (random), resulting in different outputs for multiple runs.\n",
        "- While higher probabilities increase the chances of a word being picked, less frequent words can also be generated.\n",
        "\n",
        "**4. Considerations for text generation:**\n",
        "\n",
        "- The size of *n* can affect the generated text quality. Larger *n* might capture longer-range dependencies but can lead to data sparsity and repetitive outputs.\n",
        "- The model is unable to generate text for a prompt it has not encountered in training data.\n",
        "\n",
        "In the next part of the course, you will review some of the limitations of n-gram models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
