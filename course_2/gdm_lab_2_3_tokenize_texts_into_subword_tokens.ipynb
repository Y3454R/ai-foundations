{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWLsr7Ye9RQD"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F18_qcID9TXQ"
      },
      "source": [
        "![](https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C2-white-bg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX6LqVz0IN_G"
      },
      "source": [
        "# Lab: Tokenize Texts into Subword Tokens\n",
        "\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_2/gdm_lab_2_3_tokenize_texts_into_subword_tokens.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Explore how Gemma's tokenizer splits texts into units between characters and words.\n",
        "\n",
        "10 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IcGva1R9hM4"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this lab, you will experiment with the **tokenizer** of the **Google Gemma** language model. You will explore how this model is able to deal with rare words or words that do not appear in its training data while still representing common words as their own tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA2dE2NlGEmI"
      },
      "source": [
        "### What you will learn\n",
        "\n",
        "By the end of this lab, you will understand:\n",
        "\n",
        "* What subword tokenization is.\n",
        "* How subword tokenizers handle rare and unseen words, and emojis.\n",
        "* The purpose of special tokens in tokenizers for language models.\n",
        "\n",
        "\n",
        "### Tasks\n",
        "\n",
        "In this lab, you will:\n",
        "\n",
        "* Experiment with Gemma's tokenizer to explore subword tokenization.\n",
        "* Implement a function to tokenize the made-up word \"Clusterophonexia\".\n",
        "* Inspect how Gemma handles emojis and the purpose of its special tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Mc6UzdSbSJ"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJikYa4iScuY"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over the cell and click on the `run` button to its left. The run button is the circle with the triangle (▶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or ⌘+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQunPUsRShVa"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKpR1cc0Se9B"
      },
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime → Run before__  from the menu above (or use the keyboard combination Ctrl/⌘ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6cTA8GKUba2"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will primarily work with the tokenizer from the `gemma` package.\n",
        "\n",
        "Run the following cell to import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYksWTRhUjVm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Install the custom package for this course. This also installs the gemma\n",
        "# package.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "from gemma import gm # For interacting with the Gemma tokenizer.\n",
        "# For providing feedback on your implementations.\n",
        "from ai_foundations.feedback.course_2 import subword_tokens as feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biG8FtNIUXY4"
      },
      "source": [
        "## Subword tokenization\n",
        "\n",
        "In the last activity, you saw the trade-offs between two main tokenization strategies. Character-level tokenization can handle any word but creates very long sequences and is not able to capture the inherent meaning of entire words. On the other hand, word-level tokenization is intuitive but struggles with rare or unseen words (the out-of-vocabulary problem).\n",
        "\n",
        "One method that finds a good middle ground and provides the best of both worlds is **subword tokenization**. This approach offers a clever compromise between character-level and word-level tokenization:\n",
        "\n",
        "* Frequent words (like \"the\" or \"is\") are kept as single, complete tokens.\n",
        "\n",
        "* Rare or complex words (like \"Baobab\") are broken down into smaller, meaningful sub-units.\n",
        "\n",
        "This way, the model maintains a manageable, fixed-size vocabulary while still being able to represent any word you can think of.\n",
        "\n",
        "As a first step, consider how Gemma tokenizes the following text:\n",
        "\n",
        "\"The Baobab (genus Adansonia) is\"\n",
        "\n",
        "It turns this text into the following tokens:\n",
        "\n",
        "```python\n",
        "[\"The\", \" Ba\", \"ob\", \"ab\", \" (\", \"genus\", \" Ad\", \"ans\", \"onia\", \")\", \" is\"]\n",
        "```\n",
        "You will notice that spaces are part of the tokens that form the first part of a word (or an entire word). Further, the word \"Baobab\" is broken up into three tokens. This is done using the  **byte pair encoding**  (BPE) algorithm, one of the most popular algorithms for subword tokenization, which you will implement yourself in the next lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fawc5cPQY2a"
      },
      "source": [
        "### Load and experiment with the Gemma tokenizer\n",
        "\n",
        "To gain a better intuition of how the Gemma tokenizer works, run the following cell to load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hKfYq9aIW7y"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer.\n",
        "gemma_tokenizer = gm.text.Gemma3Tokenizer()\n",
        "\n",
        "# Inspect the vocabaulary size.\n",
        "print(f\"Gemma's vocabulary consists of {gemma_tokenizer.vocab_size:,} tokens.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtmJFsV1Bc82"
      },
      "source": [
        "As you can see, Gemma has a very large vocabulary size of more than 260,000 tokens. This is because the list of unique tokens has been determined using a much larger dataset than Africa Galore. Furthermore, the tokenizer has been designed with the goal of allowing the model to capture a wide spectrum of input, from common words and frequent subword patterns, to emojis like `'☺️'`, to the character sets of different languages. With such an expansive vocabulary, Gemma can often tokenize text more precisely. This reduces instances of the meaningless `<UNK>` token and better preserves the semantic richness of the original input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz9iAKGJZxYV"
      },
      "source": [
        "#### Encoding and decoding\n",
        "\n",
        "To translate an arbitrary input text to the token IDs that Gemma can process, you can use the `encode` function of the tokenizer. This is demonstrated in the following cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqAjBWfpIp2d"
      },
      "outputs": [],
      "source": [
        "# Encode a text into token IDs.\n",
        "text = \"The Baobab (genus Adansonia) is one of the most iconic trees.\"\n",
        "\n",
        "gemma_tokens = gemma_tokenizer.encode(text)\n",
        "print(f\"Result of tokenizing the text \\\"{text}\\\":\")\n",
        "print(gemma_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txb1pahWZzqo"
      },
      "source": [
        "This process can be reversed using the `decode` method that translates the token IDs back to a text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_QO1H3VIzxf"
      },
      "outputs": [],
      "source": [
        "# Decode the tokens back to a text.\n",
        "decoded_text = gemma_tokenizer.decode(gemma_tokens)\n",
        "print(f\"Decoded sentence from tokens: {decoded_text}\\n\")\n",
        "\n",
        "# Check whether this results in the same text as the original one.\n",
        "is_equal = \"✅\" if text == decoded_text else \"❌\"\n",
        "print(\n",
        "    f\"Decoding the tokens results in the same text as the original one:\"\n",
        "    f\" {is_equal}\\n\"\n",
        ")\n",
        "\n",
        "# Decode individual tokens.\n",
        "for token in gemma_tokens:\n",
        "    decoded_token = gemma_tokenizer.decode(token)\n",
        "    print(f\"Token {token}:\\t{decoded_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-pxHjqfLVg2"
      },
      "source": [
        "### Coding Activity 1: Tokenize a made-up word\n",
        "\n",
        "As mentioned, this approach has the advantage that it can represent almost any word as a combination of multiple subword tokens.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **💻 Your task:**\n",
        ">\n",
        "> Use the `encode` and `decode` methods of the Gemma tokenizer to investigate how Gemma tokenizes the made-up word \"Clusterophonexia\".\n",
        ">\n",
        "> Store the list of token IDs in `clusterophonexia_tokens` and turn the first token ID back into a string and store it in `first_token_as_text`.\n",
        ">\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvvkjiKKKuD0"
      },
      "outputs": [],
      "source": [
        "# Set the following two variables as described in the instructions above.\n",
        "clusterophonexia_tokens =\n",
        "first_token_as_text ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6ep4yYj1wdAr"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your code\n",
        "feedback.test_gemma_subword_tokenization(clusterophonexia_tokens,\n",
        "                                         first_token_as_text,\n",
        "                                         gemma_tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC6qq17Hx4sU"
      },
      "source": [
        "As you have observed, the Gemma tokenizer can even tokenize made-up words, such as \"Clusterophonexia\". In this case the model has access to tokens it has been trained on. For example, if the meaning of \"Clusterophonexia\" is related to the word \"cluster\", then the Gemma tokenization would provide some information about what this word is supposed to mean. In this case, the blanket `<UNK>` token would not provide any information about the word's meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdBE3gXTSgb0"
      },
      "source": [
        "### Tokenizing Unicode characters\n",
        "\n",
        "Gemma's large vocabulary also includes many Unicode characters, such as emojis. Run the next cell to see how the sentence \"I am smiling ☺️!\" is tokenized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igoqKaEDSWLp"
      },
      "outputs": [],
      "source": [
        "gemma_tokens = gemma_tokenizer.encode(\"I am smiling ☺️!\")\n",
        "\n",
        "for i, token in enumerate(gemma_tokens):\n",
        "    decoded_token = gemma_tokenizer.decode(token)\n",
        "    print(f\"Token {token}:\\t{decoded_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkh0YG3-0n2-"
      },
      "source": [
        "As you can see observe, the tokenizer is also able to map emojis to token IDs. In this case, the emoji ☺️ is mapped to ID 145233."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOKcYfU71I-O"
      },
      "source": [
        "## Special tokens\n",
        "\n",
        "Beyond the regular tokens that represent words or subwords, a tokenizer's vocabulary includes special tokens. These tokens don't represent content. Instead, they provide structural information to the model, such as marking boundaries or handling sequences of different lengths.\n",
        "\n",
        "The following list includes several common special tokens:\n",
        "\n",
        "* **`<BOS>`** and **`<EOS>`**:\n",
        "\n",
        "  These stand for \"beginning of sequence\" (BOS) and \"end of sequence\" (EOS). Their primary job is to mark the start and end of a distinct piece of text. This comes with the following two advantages:\n",
        "\n",
        "  * Efficient batching: By clearly marking where each sequence begins and ends, we can feed multiple documents to the model in a single batch without having to pad them extensively.\n",
        "\n",
        "  * Dynamic generation: During text generation, the `<EOS>` token serves as a stop signal. Instead of generating a fixed number of tokens, the model can generate text until it produces an `<EOS>` token, allowing it to decide when a response is complete.\n",
        "\n",
        "* **`<PAD>`**:\n",
        "\n",
        "  As you have encountered, the padding token is used to make all input sequences in a batch the same length. Transformer models require inputs to have a fixed size, so shorter sequences are \"padded\" with this token until they match the length of the longest sequence in the batch.\n",
        "\n",
        "* **`<UNK>`**:\n",
        "\n",
        "  The unknown token, `<UNK>`, acts as a placeholder for a character or symbol that is not in the tokenizer's vocabulary. While subword tokenizers are great at representing almost any text, they can sometimes encounter a character that they have never been trained on. In these rare cases, the tokenizer will use `<UNK>` to represent it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvxqyqnD68_8"
      },
      "source": [
        "### Special tokens in Gemma\n",
        "\n",
        "As an example of how to work with special tokens, consider the implementation of special tokens in Gemma. These can be accessed through `gemma_tokenizer.special_tokens`. For example, the following two cells demonstrate how to access the BOS and EOS tokens in Gemma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7552kX6V7PJ0"
      },
      "outputs": [],
      "source": [
        "# Beginning of sentence (BOS) token.\n",
        "gemma_tokenizer.special_tokens.BOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVq6n4Gw7TXU"
      },
      "outputs": [],
      "source": [
        "# End of sentence (EOS) token.\n",
        "gemma_tokenizer.special_tokens.EOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQZbKaV57ah9"
      },
      "source": [
        "The tokenizer also supports automatically adding the BOS and EOS tokens to a sequence. This can be very useful, for example, when you are preparing data for finetuning a chatbot on prompts and model answers, to get the model to learn when it should stop generating.\n",
        "\n",
        "This can be very useful. For example, when you are preparing data for finetuning a chatbot on prompts and model answers as it enables the model to learn when it should stop generating.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyELDk4t7MiZ"
      },
      "outputs": [],
      "source": [
        "token_ids = gemma_tokenizer.encode(\"Hello world!\", add_bos=True, add_eos=True)\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f4iWtG5Qk94"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you explored how a **subword tokenizer**, such as the one used by the Gemma model, works. You explored how it splits entire texts into tokens of varying granularity, how it can represent words that are not part of its vocabulary, and how it can represent Unicode characters e.g., emojis.\n",
        "\n",
        "You also learned about the **common special tokens** `<BOS>`, `<EOS>`, `<PAD>`, and `<UNK>` and how they can be used to encode structural information in texts.\n",
        "\n",
        "In the next activities, you will learn how you can implement a subword tokenizer using the byte pair encoding (BPE) algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOqA7kn2dA6p"
      },
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities above. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "We recommend that you *only* look at the solutions after you have tried to solve the activities above *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them and then type them manually into the cell. This will help you understand where you went wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXnCirI-dC3n"
      },
      "source": [
        "### Coding Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgkJ37KydEaz"
      },
      "outputs": [],
      "source": [
        "# Add the following two lines to the cell above.\n",
        "clusterophonexia_tokens = gemma_tokenizer.encode(\"Clusterophonexia\")\n",
        "first_token_as_text = gemma_tokenizer.decode(clusterophonexia_tokens[0])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "C8Mc6UzdSbSJ",
        "qOqA7kn2dA6p",
        "lXnCirI-dC3n"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
